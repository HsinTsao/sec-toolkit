"""LLM ç›¸å…³ Schema"""
from pydantic import BaseModel, Field
from typing import Optional, List
from datetime import datetime


# ==================== LLM æä¾›å•†é…ç½® ====================

class LLMProvider(BaseModel):
    """LLM æä¾›å•†ä¿¡æ¯"""
    id: str
    name: str
    base_url: str
    models: List[str]
    default_model: str
    description: str
    icon: str


# é¢„è®¾çš„ LLM æä¾›å•†
LLM_PROVIDERS = [
    LLMProvider(
        id="openai",
        name="OpenAI",
        base_url="https://api.openai.com/v1",
        models=["gpt-4o", "gpt-4o-mini", "gpt-4-turbo", "gpt-3.5-turbo"],
        default_model="gpt-4o-mini",
        description="OpenAI å®˜æ–¹ API",
        icon="ğŸ¤–"
    ),
    LLMProvider(
        id="groq",
        name="Groq",
        base_url="https://api.groq.com/openai/v1",
        models=["llama-3.3-70b-versatile", "llama-3.1-8b-instant", "mixtral-8x7b-32768"],
        default_model="llama-3.3-70b-versatile",
        description="è¶…å¿«æ¨ç†ï¼Œå…è´¹é¢åº¦",
        icon="âš¡"
    ),
    LLMProvider(
        id="deepseek",
        name="DeepSeek",
        base_url="https://api.deepseek.com/v1",
        models=["deepseek-chat", "deepseek-coder"],
        default_model="deepseek-chat",
        description="å›½äº§æ¨¡å‹ï¼Œä»·æ ¼ä½å»‰",
        icon="ğŸ”®"
    ),
    LLMProvider(
        id="siliconflow",
        name="ç¡…åŸºæµåŠ¨",
        base_url="https://api.siliconflow.cn/v1",
        models=["Qwen/Qwen2.5-72B-Instruct", "Qwen/Qwen2.5-7B-Instruct", "deepseek-ai/DeepSeek-V3"],
        default_model="Qwen/Qwen2.5-7B-Instruct",
        description="å›½å†…å¹³å°ï¼Œå¤šæ¨¡å‹æ”¯æŒ",
        icon="ğŸŒŠ"
    ),
    LLMProvider(
        id="zhipu",
        name="æ™ºè°± AI",
        base_url="https://open.bigmodel.cn/api/paas/v4",
        models=["glm-4-flash", "glm-4", "glm-4-plus"],
        default_model="glm-4-flash",
        description="GLM ç³»åˆ—ï¼Œä¸­æ–‡ä¼˜åŒ–",
        icon="ğŸ§ "
    ),
    LLMProvider(
        id="ollama",
        name="Ollama (æœ¬åœ°)",
        base_url="http://localhost:11434/v1",
        models=["qwen2.5:7b", "llama3.2:3b", "deepseek-r1:7b"],
        default_model="qwen2.5:7b",
        description="æœ¬åœ°éƒ¨ç½²ï¼Œå®Œå…¨å…è´¹",
        icon="ğŸ¦™"
    ),
    LLMProvider(
        id="custom",
        name="è‡ªå®šä¹‰",
        base_url="",
        models=[],
        default_model="",
        description="OpenAI å…¼å®¹æ¥å£",
        icon="âš™ï¸"
    ),
]

LLM_PROVIDERS_MAP = {p.id: p for p in LLM_PROVIDERS}


# ==================== ç”¨æˆ· LLM é…ç½® ====================

class LLMConfigCreate(BaseModel):
    """åˆ›å»º/æ›´æ–° LLM é…ç½®"""
    provider_id: str = Field(..., description="æä¾›å•† ID")
    api_key: Optional[str] = Field(None, description="API Key")
    base_url: Optional[str] = Field(None, description="è‡ªå®šä¹‰ API åœ°å€")
    model: str = Field(..., description="æ¨¡å‹åç§°")


class LLMConfigResponse(BaseModel):
    """LLM é…ç½®å“åº”"""
    id: str
    provider_id: str
    api_key_set: bool  # æ˜¯å¦å·²è®¾ç½® API Keyï¼ˆä¸è¿”å›å®é™…å€¼ï¼‰
    base_url: Optional[str]
    model: str
    created_at: datetime
    updated_at: datetime
    
    class Config:
        from_attributes = True


# ==================== èŠå¤©ç›¸å…³ ====================

class ChatMessage(BaseModel):
    """èŠå¤©æ¶ˆæ¯"""
    role: str = Field(..., description="æ¶ˆæ¯è§’è‰²: user/assistant/system")
    content: str = Field(..., description="æ¶ˆæ¯å†…å®¹")


class ChatRequest(BaseModel):
    """èŠå¤©è¯·æ±‚"""
    message: str = Field(..., description="ç”¨æˆ·æ¶ˆæ¯")
    history: List[ChatMessage] = Field(default=[], description="å¯¹è¯å†å²")
    use_rag: bool = Field(default=True, description="æ˜¯å¦ä½¿ç”¨ RAG å¢å¼º")


class ChatResponse(BaseModel):
    """èŠå¤©å“åº”"""
    content: str = Field(..., description="AI å›å¤å†…å®¹")
    sources: List[str] = Field(default=[], description="å¼•ç”¨çš„çŸ¥è¯†åº“æ¥æº")


class StreamChatChunk(BaseModel):
    """æµå¼èŠå¤©å—"""
    content: str = Field(..., description="å†…å®¹ç‰‡æ®µ")
    done: bool = Field(default=False, description="æ˜¯å¦ç»“æŸ")

