"""LLM é…ç½®å’ŒèŠå¤© API"""
from fastapi import APIRouter, Depends, HTTPException
from fastapi.responses import StreamingResponse
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, or_
import httpx
import json
import logging
from typing import AsyncGenerator, List, Optional

logger = logging.getLogger(__name__)

from ...database import get_db
from ...models import User, UserLLMConfig, KnowledgeItem
from ...schemas.llm import (
    LLMConfigCreate, 
    LLMConfigResponse, 
    ChatRequest, 
    ChatResponse,
    LLM_PROVIDERS,
    LLM_PROVIDERS_MAP,
    LLMProvider
)
from ...schemas.knowledge import RAGChatRequest, RAGSource
from ..deps import get_current_user

router = APIRouter(prefix="/llm", tags=["LLM"])


# ==================== LLM æä¾›å•† ====================

@router.get("/providers", response_model=list[LLMProvider])
async def get_llm_providers():
    """è·å–æ‰€æœ‰ LLM æä¾›å•†åˆ—è¡¨"""
    return LLM_PROVIDERS


# ==================== ç”¨æˆ· LLM é…ç½® ====================

@router.get("/config", response_model=LLMConfigResponse)
async def get_llm_config(
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """è·å–å½“å‰ç”¨æˆ·çš„ LLM é…ç½®"""
    result = await db.execute(
        select(UserLLMConfig).where(UserLLMConfig.user_id == current_user.id)
    )
    config = result.scalar_one_or_none()
    
    if not config:
        raise HTTPException(status_code=404, detail="LLM é…ç½®æœªè®¾ç½®")
    
    return LLMConfigResponse(
        id=config.id,
        provider_id=config.provider_id,
        api_key_set=bool(config.api_key),
        base_url=config.base_url,
        model=config.model,
        created_at=config.created_at,
        updated_at=config.updated_at
    )


@router.put("/config", response_model=LLMConfigResponse)
async def update_llm_config(
    config_data: LLMConfigCreate,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """æ›´æ–°å½“å‰ç”¨æˆ·çš„ LLM é…ç½®"""
    result = await db.execute(
        select(UserLLMConfig).where(UserLLMConfig.user_id == current_user.id)
    )
    config = result.scalar_one_or_none()
    
    base_url = config_data.base_url
    if not base_url and config_data.provider_id in LLM_PROVIDERS_MAP:
        base_url = LLM_PROVIDERS_MAP[config_data.provider_id].base_url
    
    if config:
        config.provider_id = config_data.provider_id
        if config_data.api_key is not None:
            config.api_key = config_data.api_key
        config.base_url = base_url
        config.model = config_data.model
    else:
        config = UserLLMConfig(
            user_id=current_user.id,
            provider_id=config_data.provider_id,
            api_key=config_data.api_key,
            base_url=base_url,
            model=config_data.model
        )
        db.add(config)
    
    await db.flush()
    await db.refresh(config)
    
    return LLMConfigResponse(
        id=config.id,
        provider_id=config.provider_id,
        api_key_set=bool(config.api_key),
        base_url=config.base_url,
        model=config.model,
        created_at=config.created_at,
        updated_at=config.updated_at
    )


@router.delete("/config")
async def delete_llm_config(
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """åˆ é™¤å½“å‰ç”¨æˆ·çš„ LLM é…ç½®"""
    result = await db.execute(
        select(UserLLMConfig).where(UserLLMConfig.user_id == current_user.id)
    )
    config = result.scalar_one_or_none()
    
    if config:
        await db.delete(config)
    
    return {"message": "é…ç½®å·²åˆ é™¤"}


# ==================== èŠå¤© API ====================

# åŸºç¡€ç³»ç»Ÿæç¤ºè¯
BASE_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ Web å®‰å…¨åˆ†æåŠ©æ‰‹ã€‚

## ä½ çš„èƒ½åŠ›
- åˆ†æ HTTP è¯·æ±‚/å“åº”ä¸­çš„å®‰å…¨é—®é¢˜
- è¯†åˆ«å¸¸è§æ¼æ´ï¼ˆSQLæ³¨å…¥ã€XSSã€CSRFã€SSRFç­‰ï¼‰
- ç”Ÿæˆæµ‹è¯• payload
- æä¾›ä¿®å¤å»ºè®®

## å›å¤æ ¼å¼
å½“ç”¨æˆ·æä¾› HTTP è¯·æ±‚æˆ–è¯¢é—®å®‰å…¨é—®é¢˜æ—¶ï¼Œè¯·æŒ‰ä»¥ä¸‹ç»“æ„å›å¤ï¼š

### 1. ğŸ” åˆ†æ
ç®€è¦åˆ†æè¯·æ±‚ç»“æ„å’Œæ½œåœ¨é£é™©ç‚¹

### 2. ğŸ¯ æ½œåœ¨æ¼æ´
åˆ—å‡ºå¯èƒ½å­˜åœ¨çš„æ¼æ´ç±»å‹å’Œé£é™©ç­‰çº§

### 3. ğŸ§ª æµ‹è¯•å»ºè®®
æä¾›å…·ä½“çš„æµ‹è¯• payloadï¼ˆå¯ç›´æ¥å¤åˆ¶ä½¿ç”¨ï¼‰

### 4. ğŸ›¡ï¸ ä¿®å¤å»ºè®®
å¦‚æœå‘ç°é—®é¢˜ï¼Œæä¾›ä¿®å¤æ–¹æ¡ˆ

## é‡è¦åŸåˆ™
- åªåœ¨ç”¨æˆ·æœ‰æˆæƒçš„æƒ…å†µä¸‹è¿›è¡Œæµ‹è¯•å»ºè®®
- æä¾›å¯æ“ä½œçš„å…·ä½“å»ºè®®
- è§£é‡Šæ¸…æ¥šæ¼æ´åŸç†"""

# RAG å¢å¼ºç³»ç»Ÿæç¤ºè¯æ¨¡æ¿
RAG_SYSTEM_PROMPT_TEMPLATE = """{base_prompt}

## ç”¨æˆ·çŸ¥è¯†åº“å‚è€ƒ
ä»¥ä¸‹æ˜¯ä»ç”¨æˆ·çŸ¥è¯†åº“ä¸­æ£€ç´¢åˆ°çš„ç›¸å…³å†…å®¹ï¼Œè¯·ä¼˜å…ˆå‚è€ƒè¿™äº›å†…å®¹æ¥å›ç­”é—®é¢˜ï¼š

{knowledge_context}

---
è¯·åŸºäºä»¥ä¸ŠçŸ¥è¯†åº“å†…å®¹å’Œä½ çš„ä¸“ä¸šçŸ¥è¯†æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœçŸ¥è¯†åº“ä¸­æœ‰ç›¸å…³å†…å®¹ï¼Œè¯·åœ¨å›ç­”ä¸­å¼•ç”¨ã€‚"""


async def get_user_llm_config(user_id: str, db: AsyncSession) -> UserLLMConfig:
    """è·å–ç”¨æˆ·çš„ LLM é…ç½®"""
    result = await db.execute(
        select(UserLLMConfig).where(UserLLMConfig.user_id == user_id)
    )
    config = result.scalar_one_or_none()
    
    if not config:
        raise HTTPException(
            status_code=400, 
            detail="è¯·å…ˆåœ¨è®¾ç½®ä¸­é…ç½® LLM API Key"
        )
    
    if not config.api_key and config.provider_id != "ollama":
        raise HTTPException(
            status_code=400,
            detail="è¯·å…ˆé…ç½® API Key"
        )
    
    return config


async def search_knowledge_base(
    user_id: str,
    query: str,
    source_types: List[str],
    limit: int,
    db: AsyncSession
) -> List[KnowledgeItem]:
    """æœç´¢çŸ¥è¯†åº“"""
    if not query.strip():
        return []
    
    # å…ˆç»Ÿè®¡ç”¨æˆ·çŸ¥è¯†åº“æ€»æ•°ï¼ˆè°ƒè¯•ç”¨ï¼‰
    total_query = select(KnowledgeItem).where(
        KnowledgeItem.user_id == user_id,
        KnowledgeItem.is_enabled == True,
    )
    if source_types:
        total_query = total_query.where(KnowledgeItem.source_type.in_(source_types))
    total_result = await db.execute(total_query)
    all_items = list(total_result.scalars().all())
    print(f"ğŸ“Š [RAG] ç”¨æˆ·çŸ¥è¯†åº“æ€»æ•°: {len(all_items)} æ¡ (æ¥æºç±»å‹: {source_types})")
    if all_items:
        print(f"ğŸ“Š [RAG] çŸ¥è¯†åº“æ ‡é¢˜: {[item.title[:30] for item in all_items[:5]]}...")
    
    # æ„å»ºæŸ¥è¯¢
    db_query = select(KnowledgeItem).where(
        KnowledgeItem.user_id == user_id,
        KnowledgeItem.is_enabled == True,
    )
    
    # ç­›é€‰æ¥æºç±»å‹
    if source_types:
        db_query = db_query.where(KnowledgeItem.source_type.in_(source_types))
    
    # ä½¿ç”¨ jieba è¿›è¡Œä¸­æ–‡åˆ†è¯
    import jieba
    import jieba.analyse
    
    # ä½¿ç”¨ TF-IDF æå–å…³é”®è¯ï¼ˆæ›´æ™ºèƒ½çš„è¯­ä¹‰åˆ†å‰²ï¼‰
    keywords = jieba.analyse.extract_tags(query, topK=8, withWeight=False)
    
    # è¡¥å……ï¼šæå–è‹±æ–‡å•è¯ï¼ˆjieba å¯¹è‹±æ–‡å¤„ç†è¾ƒå¼±ï¼‰
    import re
    eng_words = re.findall(r'[a-zA-Z]{2,}', query)
    keywords = list(dict.fromkeys(eng_words + keywords))[:10]
    
    print(f"ğŸ”‘ [RAG] jieba åˆ†è¯å…³é”®è¯: {keywords}")
    
    if keywords:
        # æ„å»º OR æ¡ä»¶ï¼šæ ‡é¢˜æˆ–å†…å®¹åŒ…å«ä»»æ„å…³é”®è¯
        conditions = []
        for keyword in keywords[:5]:  # é™åˆ¶å…³é”®è¯æ•°é‡
            pattern = f"%{keyword}%"
            conditions.append(KnowledgeItem.title.ilike(pattern))
            conditions.append(KnowledgeItem.content.ilike(pattern))
        
        db_query = db_query.where(or_(*conditions))
    
    db_query = db_query.limit(limit)
    
    result = await db.execute(db_query)
    return list(result.scalars().all())


def build_knowledge_context(items: List[KnowledgeItem]) -> tuple[str, List[RAGSource]]:
    """æ„å»ºçŸ¥è¯†åº“ä¸Šä¸‹æ–‡å’Œæ¥æºåˆ—è¡¨ï¼ˆä¼˜å…ˆä½¿ç”¨æ‘˜è¦ï¼‰"""
    if not items:
        return "", []
    
    context_parts = []
    sources = []
    
    for i, item in enumerate(items, 1):
        # æ¥æºç±»å‹æ ‡è®°
        type_emoji = {"note": "ğŸ“", "bookmark": "ğŸ”—", "file": "ğŸ“„"}.get(item.source_type, "ğŸ“‹")
        
        # ä¼˜å…ˆä½¿ç”¨æ‘˜è¦ï¼Œæ²¡æœ‰æ‘˜è¦åˆ™ä½¿ç”¨å†…å®¹é¢„è§ˆ
        if item.summary:
            # æœ‰æ‘˜è¦æ—¶ï¼Œä½¿ç”¨æ‘˜è¦ä½œä¸ºä¸»è¦å†…å®¹
            content_text = f"**æ‘˜è¦:** {item.summary}"
            # å¦‚æœå†…å®¹ä¸å¤ªé•¿ï¼Œä¹Ÿé™„å¸¦éƒ¨åˆ†å†…å®¹
            if item.content and len(item.content) <= 500:
                content_text += f"\n\n**è¯¦æƒ…:** {item.content}"
            elif item.content:
                content_text += f"\n\n**è¯¦æƒ…é¢„è§ˆ:** {item.content[:300]}..."
        else:
            # æ²¡æœ‰æ‘˜è¦æ—¶ï¼Œä½¿ç”¨å†…å®¹é¢„è§ˆ
            content_text = item.content[:1000] if item.content else item.title
        
        context_parts.append(f"""### {type_emoji} [{i}] {item.title}
{content_text}
""")
        
        # æ„å»ºæ¥æºï¼ˆsnippet ä¼˜å…ˆä½¿ç”¨æ‘˜è¦ï¼‰
        snippet = item.summary if item.summary else (item.content[:200] if item.content else "")
        sources.append(RAGSource(
            source_type=item.source_type,
            source_id=item.source_id,
            title=item.title,
            snippet=snippet,
            url=item.url,
        ))
    
    return "\n".join(context_parts), sources


@router.post("/chat", response_model=ChatResponse)
async def chat(
    request: ChatRequest,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """èŠå¤©æ¥å£ï¼ˆéæµå¼ï¼‰"""
    config = await get_user_llm_config(current_user.id, db)
    
    # RAG æ£€ç´¢
    sources = []
    system_prompt = BASE_SYSTEM_PROMPT
    
    if request.use_rag:
        # æœç´¢çŸ¥è¯†åº“
        knowledge_items = await search_knowledge_base(
            user_id=current_user.id,
            query=request.message,
            source_types=["note", "bookmark", "file"],
            limit=5,
            db=db,
        )
        
        if knowledge_items:
            knowledge_context, sources = build_knowledge_context(knowledge_items)
            system_prompt = RAG_SYSTEM_PROMPT_TEMPLATE.format(
                base_prompt=BASE_SYSTEM_PROMPT,
                knowledge_context=knowledge_context,
            )
    
    # æ„å»ºæ¶ˆæ¯
    messages = [{"role": "system", "content": system_prompt}]
    
    for msg in request.history:
        messages.append({"role": msg.role, "content": msg.content})
    
    messages.append({"role": "user", "content": request.message})
    
    # è°ƒç”¨ LLM
    try:
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                f"{config.base_url}/chat/completions",
                headers={
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {config.api_key}"
                },
                json={
                    "model": config.model,
                    "messages": messages,
                    "stream": False
                }
            )
            
            if response.status_code != 200:
                error_detail = response.text
                try:
                    error_json = response.json()
                    error_detail = error_json.get("error", {}).get("message", error_detail)
                except:
                    pass
                raise HTTPException(status_code=response.status_code, detail=error_detail)
            
            result = response.json()
            content = result["choices"][0]["message"]["content"]
            
            return ChatResponse(
                content=content, 
                sources=[s.model_dump() for s in sources]
            )
            
    except httpx.TimeoutException:
        raise HTTPException(status_code=504, detail="LLM è¯·æ±‚è¶…æ—¶")
    except httpx.RequestError as e:
        raise HTTPException(status_code=502, detail=f"LLM è¯·æ±‚å¤±è´¥: {str(e)}")


@router.post("/chat/stream")
async def chat_stream(
    request: RAGChatRequest,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """èŠå¤©æ¥å£ï¼ˆæµå¼ï¼Œæ”¯æŒ RAGï¼‰"""
    config = await get_user_llm_config(current_user.id, db)
    
    # RAG æ£€ç´¢
    sources: List[RAGSource] = []
    system_prompt = BASE_SYSTEM_PROMPT
    
    print(f"ğŸ” [Chat] ç”¨æˆ·={current_user.username}, çŸ¥è¯†åº“={request.use_knowledge}, æ¥æºç±»å‹={request.knowledge_sources}")
    
    if request.use_knowledge:
        # æœç´¢çŸ¥è¯†åº“
        knowledge_items = await search_knowledge_base(
            user_id=current_user.id,
            query=request.message,
            source_types=request.knowledge_sources,
            limit=request.max_results,
            db=db,
        )
        
        print(f"ğŸ“š [Chat] çŸ¥è¯†åº“æ£€ç´¢ç»“æœ: {len(knowledge_items)} æ¡")
        
        if knowledge_items:
            knowledge_context, sources = build_knowledge_context(knowledge_items)
            system_prompt = RAG_SYSTEM_PROMPT_TEMPLATE.format(
                base_prompt=BASE_SYSTEM_PROMPT,
                knowledge_context=knowledge_context,
            )
            print(f"ğŸ“– [Chat] RAG æ¥æº: {[s.title for s in sources]}")
        else:
            print("âš ï¸ [Chat] çŸ¥è¯†åº“æœªæ£€ç´¢åˆ°ç›¸å…³å†…å®¹")
    
    # æ„å»ºæ¶ˆæ¯
    messages = [{"role": "system", "content": system_prompt}]
    
    for msg in request.history:
        messages.append({"role": msg.get("role", "user"), "content": msg.get("content", "")})
    
    messages.append({"role": "user", "content": request.message})
    
    async def generate() -> AsyncGenerator[str, None]:
        # å…ˆå‘é€æ¥æºä¿¡æ¯
        if sources:
            yield f"data: {json.dumps({'sources': [s.model_dump() for s in sources]})}\n\n"
        
        try:
            async with httpx.AsyncClient(timeout=120.0) as client:
                async with client.stream(
                    "POST",
                    f"{config.base_url}/chat/completions",
                    headers={
                        "Content-Type": "application/json",
                        "Authorization": f"Bearer {config.api_key}"
                    },
                    json={
                        "model": config.model,
                        "messages": messages,
                        "stream": True
                    }
                ) as response:
                    if response.status_code != 200:
                        error_text = await response.aread()
                        yield f"data: {json.dumps({'error': error_text.decode()})}\n\n"
                        return
                    
                    async for line in response.aiter_lines():
                        if line.startswith("data: "):
                            data = line[6:]
                            if data == "[DONE]":
                                yield f"data: {json.dumps({'done': True})}\n\n"
                                break
                            try:
                                chunk = json.loads(data)
                                content = chunk.get("choices", [{}])[0].get("delta", {}).get("content", "")
                                if content:
                                    yield f"data: {json.dumps({'content': content})}\n\n"
                            except json.JSONDecodeError:
                                pass
                                
        except Exception as e:
            yield f"data: {json.dumps({'error': str(e)})}\n\n"
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        }
    )
