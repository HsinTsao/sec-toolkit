"""LLM é…ç½®å’ŒèŠå¤© API"""
from fastapi import APIRouter, Depends, HTTPException
from fastapi.responses import StreamingResponse
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, or_
import httpx
import json
import logging
from typing import AsyncGenerator, List, Optional, Any, Dict

logger = logging.getLogger(__name__)

from ...database import get_db
from ...config import settings
from ...models import User, UserLLMConfig, KnowledgeItem
from ...schemas.llm import (
    LLMConfigCreate, 
    LLMConfigResponse, 
    ChatRequest, 
    ChatResponse,
    LLM_PROVIDERS,
    LLM_PROVIDERS_MAP,
    LLMProvider
)
from ...schemas.knowledge import RAGChatRequest, RAGSource
from ..deps import get_current_user

# Agent Tool Calling
from ...agent import tool_registry, tool_executor
from ...agent.audit import AuditLogger, get_user_sessions, get_session_logs, get_session_detail, get_user_stats

router = APIRouter(prefix="/llm", tags=["LLM"])


# ==================== LLM æä¾›å•† ====================

@router.get("/providers", response_model=list[LLMProvider])
async def get_llm_providers():
    """è·å–æ‰€æœ‰ LLM æä¾›å•†åˆ—è¡¨"""
    return LLM_PROVIDERS


# ==================== ç”¨æˆ· LLM é…ç½® ====================

@router.get("/config", response_model=LLMConfigResponse)
async def get_llm_config(
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """è·å–å½“å‰ç”¨æˆ·çš„ LLM é…ç½®"""
    result = await db.execute(
        select(UserLLMConfig).where(UserLLMConfig.user_id == current_user.id)
    )
    config = result.scalar_one_or_none()
    
    if not config:
        raise HTTPException(status_code=404, detail="LLM é…ç½®æœªè®¾ç½®")
    
    return LLMConfigResponse(
        id=config.id,
        provider_id=config.provider_id,
        api_key_set=bool(config.api_key),
        base_url=config.base_url,
        model=config.model,
        created_at=config.created_at,
        updated_at=config.updated_at
    )


@router.put("/config", response_model=LLMConfigResponse)
async def update_llm_config(
    config_data: LLMConfigCreate,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """æ›´æ–°å½“å‰ç”¨æˆ·çš„ LLM é…ç½®"""
    result = await db.execute(
        select(UserLLMConfig).where(UserLLMConfig.user_id == current_user.id)
    )
    config = result.scalar_one_or_none()
    
    base_url = config_data.base_url
    if not base_url and config_data.provider_id in LLM_PROVIDERS_MAP:
        base_url = LLM_PROVIDERS_MAP[config_data.provider_id].base_url
    
    if config:
        config.provider_id = config_data.provider_id
        if config_data.api_key is not None:
            config.api_key = config_data.api_key
        config.base_url = base_url
        config.model = config_data.model
    else:
        config = UserLLMConfig(
            user_id=current_user.id,
            provider_id=config_data.provider_id,
            api_key=config_data.api_key,
            base_url=base_url,
            model=config_data.model
        )
        db.add(config)
    
    await db.flush()
    await db.refresh(config)
    
    return LLMConfigResponse(
        id=config.id,
        provider_id=config.provider_id,
        api_key_set=bool(config.api_key),
        base_url=config.base_url,
        model=config.model,
        created_at=config.created_at,
        updated_at=config.updated_at
    )


@router.delete("/config")
async def delete_llm_config(
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """åˆ é™¤å½“å‰ç”¨æˆ·çš„ LLM é…ç½®"""
    result = await db.execute(
        select(UserLLMConfig).where(UserLLMConfig.user_id == current_user.id)
    )
    config = result.scalar_one_or_none()
    
    if config:
        await db.delete(config)
    
    return {"message": "é…ç½®å·²åˆ é™¤"}


# ==================== èŠå¤© API ====================

# åŸºç¡€ç³»ç»Ÿæç¤ºè¯
BASE_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ Web å®‰å…¨åˆ†æåŠ©æ‰‹ã€‚

## ä½ çš„èƒ½åŠ›
- åˆ†æ HTTP è¯·æ±‚/å“åº”ä¸­çš„å®‰å…¨é—®é¢˜
- è¯†åˆ«å¸¸è§æ¼æ´ï¼ˆSQLæ³¨å…¥ã€XSSã€CSRFã€SSRFç­‰ï¼‰
- ç”Ÿæˆæµ‹è¯• payload
- æä¾›ä¿®å¤å»ºè®®

## å›å¤æ ¼å¼
å½“ç”¨æˆ·æä¾› HTTP è¯·æ±‚æˆ–è¯¢é—®å®‰å…¨é—®é¢˜æ—¶ï¼Œè¯·æŒ‰ä»¥ä¸‹ç»“æ„å›å¤ï¼š

### 1. ğŸ” åˆ†æ
ç®€è¦åˆ†æè¯·æ±‚ç»“æ„å’Œæ½œåœ¨é£é™©ç‚¹

### 2. ğŸ¯ æ½œåœ¨æ¼æ´
åˆ—å‡ºå¯èƒ½å­˜åœ¨çš„æ¼æ´ç±»å‹å’Œé£é™©ç­‰çº§

### 3. ğŸ§ª æµ‹è¯•å»ºè®®
æä¾›å…·ä½“çš„æµ‹è¯• payloadï¼ˆå¯ç›´æ¥å¤åˆ¶ä½¿ç”¨ï¼‰

### 4. ğŸ›¡ï¸ ä¿®å¤å»ºè®®
å¦‚æœå‘ç°é—®é¢˜ï¼Œæä¾›ä¿®å¤æ–¹æ¡ˆ

## é‡è¦åŸåˆ™
- åªåœ¨ç”¨æˆ·æœ‰æˆæƒçš„æƒ…å†µä¸‹è¿›è¡Œæµ‹è¯•å»ºè®®
- æä¾›å¯æ“ä½œçš„å…·ä½“å»ºè®®
- è§£é‡Šæ¸…æ¥šæ¼æ´åŸç†"""

# RAG å¢å¼ºç³»ç»Ÿæç¤ºè¯æ¨¡æ¿
RAG_SYSTEM_PROMPT_TEMPLATE = """{base_prompt}

## ç”¨æˆ·çŸ¥è¯†åº“å‚è€ƒ
ä»¥ä¸‹æ˜¯ä»ç”¨æˆ·çŸ¥è¯†åº“ä¸­æ£€ç´¢åˆ°çš„ç›¸å…³å†…å®¹ï¼Œè¯·ä¼˜å…ˆå‚è€ƒè¿™äº›å†…å®¹æ¥å›ç­”é—®é¢˜ï¼š

{knowledge_context}

---
è¯·åŸºäºä»¥ä¸ŠçŸ¥è¯†åº“å†…å®¹å’Œä½ çš„ä¸“ä¸šçŸ¥è¯†æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœçŸ¥è¯†åº“ä¸­æœ‰ç›¸å…³å†…å®¹ï¼Œè¯·åœ¨å›ç­”ä¸­å¼•ç”¨ã€‚"""


class DefaultLLMConfig:
    """é»˜è®¤ LLM é…ç½®ï¼ˆç”¨äºç³»ç»Ÿé»˜è®¤é…ç½®ï¼‰"""
    def __init__(self, provider_id: str, api_key: str, base_url: str, model: str):
        self.provider_id = provider_id
        self.api_key = api_key
        self.base_url = base_url
        self.model = model


def get_default_llm_config() -> Optional[DefaultLLMConfig]:
    """è·å–ç³»ç»Ÿé»˜è®¤ LLM é…ç½®"""
    if not settings.DEFAULT_LLM_API_KEY:
        return None
    
    provider_id = settings.DEFAULT_LLM_PROVIDER or "deepseek"
    
    # è·å– base_urlï¼šä¼˜å…ˆä½¿ç”¨é…ç½®çš„ï¼Œå¦åˆ™ä½¿ç”¨æä¾›å•†é»˜è®¤çš„
    base_url = settings.DEFAULT_LLM_BASE_URL
    if not base_url and provider_id in LLM_PROVIDERS_MAP:
        base_url = LLM_PROVIDERS_MAP[provider_id].base_url
    
    # è·å– modelï¼šä¼˜å…ˆä½¿ç”¨é…ç½®çš„ï¼Œå¦åˆ™ä½¿ç”¨æä¾›å•†é»˜è®¤çš„
    model = settings.DEFAULT_LLM_MODEL
    if not model and provider_id in LLM_PROVIDERS_MAP:
        model = LLM_PROVIDERS_MAP[provider_id].default_model
    
    if not base_url or not model:
        return None
    
    return DefaultLLMConfig(
        provider_id=provider_id,
        api_key=settings.DEFAULT_LLM_API_KEY,
        base_url=base_url,
        model=model
    )


async def get_user_llm_config(user_id: str, db: AsyncSession):
    """è·å–ç”¨æˆ·çš„ LLM é…ç½®ï¼Œå¦‚æœç”¨æˆ·æœªé…ç½®åˆ™å°è¯•ä½¿ç”¨ç³»ç»Ÿé»˜è®¤é…ç½®"""
    result = await db.execute(
        select(UserLLMConfig).where(UserLLMConfig.user_id == user_id)
    )
    config = result.scalar_one_or_none()
    
    # å¦‚æœç”¨æˆ·æœ‰é…ç½®ä¸”æœ‰ API Keyï¼ˆæˆ–è€…æ˜¯ Ollamaï¼‰ï¼Œä½¿ç”¨ç”¨æˆ·é…ç½®
    if config and (config.api_key or config.provider_id == "ollama"):
        return config
    
    # å°è¯•ä½¿ç”¨ç³»ç»Ÿé»˜è®¤é…ç½®
    default_config = get_default_llm_config()
    if default_config:
        logger.info(f"ç”¨æˆ· {user_id} ä½¿ç”¨ç³»ç»Ÿé»˜è®¤ LLM é…ç½® (provider={default_config.provider_id})")
        return default_config
    
    # éƒ½æ²¡æœ‰é…ç½®ï¼ŒæŠ¥é”™
    if not config:
        raise HTTPException(
            status_code=400, 
            detail="è¯·å…ˆåœ¨è®¾ç½®ä¸­é…ç½® LLM API Key"
        )
    
    raise HTTPException(
        status_code=400,
        detail="è¯·å…ˆé…ç½® API Key"
    )


async def search_knowledge_base(
    user_id: str,
    query: str,
    source_types: List[str],
    limit: int,
    db: AsyncSession
) -> List[KnowledgeItem]:
    """æœç´¢çŸ¥è¯†åº“"""
    if not query.strip():
        return []
    
    # å…ˆç»Ÿè®¡ç”¨æˆ·çŸ¥è¯†åº“æ€»æ•°ï¼ˆè°ƒè¯•ç”¨ï¼‰
    total_query = select(KnowledgeItem).where(
        KnowledgeItem.user_id == user_id,
        KnowledgeItem.is_enabled == True,
    )
    if source_types:
        total_query = total_query.where(KnowledgeItem.source_type.in_(source_types))
    total_result = await db.execute(total_query)
    all_items = list(total_result.scalars().all())
    print(f"ğŸ“Š [RAG] ç”¨æˆ·çŸ¥è¯†åº“æ€»æ•°: {len(all_items)} æ¡ (æ¥æºç±»å‹: {source_types})")
    if all_items:
        print(f"ğŸ“Š [RAG] çŸ¥è¯†åº“æ ‡é¢˜: {[item.title[:30] for item in all_items[:5]]}...")
    
    # æ„å»ºæŸ¥è¯¢
    db_query = select(KnowledgeItem).where(
        KnowledgeItem.user_id == user_id,
        KnowledgeItem.is_enabled == True,
    )
    
    # ç­›é€‰æ¥æºç±»å‹
    if source_types:
        db_query = db_query.where(KnowledgeItem.source_type.in_(source_types))
    
    # ä½¿ç”¨ jieba è¿›è¡Œä¸­æ–‡åˆ†è¯
    import jieba
    import jieba.analyse
    
    # ä½¿ç”¨ TF-IDF æå–å…³é”®è¯ï¼ˆæ›´æ™ºèƒ½çš„è¯­ä¹‰åˆ†å‰²ï¼‰
    keywords = jieba.analyse.extract_tags(query, topK=8, withWeight=False)
    
    # è¡¥å……ï¼šæå–è‹±æ–‡å•è¯ï¼ˆjieba å¯¹è‹±æ–‡å¤„ç†è¾ƒå¼±ï¼‰
    import re
    eng_words = re.findall(r'[a-zA-Z]{2,}', query)
    keywords = list(dict.fromkeys(eng_words + keywords))[:10]
    
    print(f"ğŸ”‘ [RAG] jieba åˆ†è¯å…³é”®è¯: {keywords}")
    
    if keywords:
        # æ„å»º OR æ¡ä»¶ï¼šæ ‡é¢˜æˆ–å†…å®¹åŒ…å«ä»»æ„å…³é”®è¯
        conditions = []
        for keyword in keywords[:5]:  # é™åˆ¶å…³é”®è¯æ•°é‡
            pattern = f"%{keyword}%"
            conditions.append(KnowledgeItem.title.ilike(pattern))
            conditions.append(KnowledgeItem.content.ilike(pattern))
        
        db_query = db_query.where(or_(*conditions))
    
    db_query = db_query.limit(limit)
    
    result = await db.execute(db_query)
    return list(result.scalars().all())


def build_knowledge_context(items: List[KnowledgeItem]) -> tuple[str, List[RAGSource]]:
    """æ„å»ºçŸ¥è¯†åº“ä¸Šä¸‹æ–‡å’Œæ¥æºåˆ—è¡¨ï¼ˆä¼˜å…ˆä½¿ç”¨æ‘˜è¦ï¼‰"""
    if not items:
        return "", []
    
    context_parts = []
    sources = []
    
    for i, item in enumerate(items, 1):
        # æ¥æºç±»å‹æ ‡è®°
        type_emoji = {"note": "ğŸ“", "bookmark": "ğŸ”—", "file": "ğŸ“„"}.get(item.source_type, "ğŸ“‹")
        
        # ä¼˜å…ˆä½¿ç”¨æ‘˜è¦ï¼Œæ²¡æœ‰æ‘˜è¦åˆ™ä½¿ç”¨å†…å®¹é¢„è§ˆ
        if item.summary:
            # æœ‰æ‘˜è¦æ—¶ï¼Œä½¿ç”¨æ‘˜è¦ä½œä¸ºä¸»è¦å†…å®¹
            content_text = f"**æ‘˜è¦:** {item.summary}"
            # å¦‚æœå†…å®¹ä¸å¤ªé•¿ï¼Œä¹Ÿé™„å¸¦éƒ¨åˆ†å†…å®¹
            if item.content and len(item.content) <= 500:
                content_text += f"\n\n**è¯¦æƒ…:** {item.content}"
            elif item.content:
                content_text += f"\n\n**è¯¦æƒ…é¢„è§ˆ:** {item.content[:300]}..."
        else:
            # æ²¡æœ‰æ‘˜è¦æ—¶ï¼Œä½¿ç”¨å†…å®¹é¢„è§ˆ
            content_text = item.content[:1000] if item.content else item.title
        
        context_parts.append(f"""### {type_emoji} [{i}] {item.title}
{content_text}
""")
        
        # æ„å»ºæ¥æºï¼ˆsnippet ä¼˜å…ˆä½¿ç”¨æ‘˜è¦ï¼‰
        snippet = item.summary if item.summary else (item.content[:200] if item.content else "")
        sources.append(RAGSource(
            source_type=item.source_type,
            source_id=item.source_id,
            title=item.title,
            snippet=snippet,
            url=item.url,
        ))
    
    return "\n".join(context_parts), sources


@router.post("/chat", response_model=ChatResponse)
async def chat(
    request: ChatRequest,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """èŠå¤©æ¥å£ï¼ˆéæµå¼ï¼‰"""
    config = await get_user_llm_config(current_user.id, db)
    
    # RAG æ£€ç´¢
    sources = []
    system_prompt = BASE_SYSTEM_PROMPT
    
    if request.use_rag:
        # æœç´¢çŸ¥è¯†åº“
        knowledge_items = await search_knowledge_base(
            user_id=current_user.id,
            query=request.message,
            source_types=["note", "bookmark", "file"],
            limit=5,
            db=db,
        )
        
        if knowledge_items:
            knowledge_context, sources = build_knowledge_context(knowledge_items)
            system_prompt = RAG_SYSTEM_PROMPT_TEMPLATE.format(
                base_prompt=BASE_SYSTEM_PROMPT,
                knowledge_context=knowledge_context,
            )
    
    # æ„å»ºæ¶ˆæ¯
    messages = [{"role": "system", "content": system_prompt}]
    
    for msg in request.history:
        messages.append({"role": msg.role, "content": msg.content})
    
    messages.append({"role": "user", "content": request.message})
    
    # è°ƒç”¨ LLM
    try:
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                f"{config.base_url}/chat/completions",
                headers={
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {config.api_key}"
                },
                json={
                    "model": config.model,
                    "messages": messages,
                    "stream": False
                }
            )
            
            if response.status_code != 200:
                error_detail = response.text
                try:
                    error_json = response.json()
                    error_detail = error_json.get("error", {}).get("message", error_detail)
                except:
                    pass
                raise HTTPException(status_code=response.status_code, detail=error_detail)
            
            result = response.json()
            content = result["choices"][0]["message"]["content"]
            
            return ChatResponse(
                content=content, 
                sources=[s.model_dump() for s in sources]
            )
            
    except httpx.TimeoutException:
        raise HTTPException(status_code=504, detail="LLM è¯·æ±‚è¶…æ—¶")
    except httpx.RequestError as e:
        raise HTTPException(status_code=502, detail=f"LLM è¯·æ±‚å¤±è´¥: {str(e)}")


@router.post("/chat/stream")
async def chat_stream(
    request: RAGChatRequest,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """èŠå¤©æ¥å£ï¼ˆæµå¼ï¼Œæ”¯æŒ RAGï¼‰"""
    config = await get_user_llm_config(current_user.id, db)
    
    # RAG æ£€ç´¢
    sources: List[RAGSource] = []
    system_prompt = BASE_SYSTEM_PROMPT
    
    print(f"ğŸ” [Chat] ç”¨æˆ·={current_user.username}, çŸ¥è¯†åº“={request.use_knowledge}, æ¥æºç±»å‹={request.knowledge_sources}")
    
    if request.use_knowledge:
        # æœç´¢çŸ¥è¯†åº“
        knowledge_items = await search_knowledge_base(
            user_id=current_user.id,
            query=request.message,
            source_types=request.knowledge_sources,
            limit=request.max_results,
            db=db,
        )
        
        print(f"ğŸ“š [Chat] çŸ¥è¯†åº“æ£€ç´¢ç»“æœ: {len(knowledge_items)} æ¡")
        
        if knowledge_items:
            knowledge_context, sources = build_knowledge_context(knowledge_items)
            system_prompt = RAG_SYSTEM_PROMPT_TEMPLATE.format(
                base_prompt=BASE_SYSTEM_PROMPT,
                knowledge_context=knowledge_context,
            )
            print(f"ğŸ“– [Chat] RAG æ¥æº: {[s.title for s in sources]}")
        else:
            print("âš ï¸ [Chat] çŸ¥è¯†åº“æœªæ£€ç´¢åˆ°ç›¸å…³å†…å®¹")
    
    # æ„å»ºæ¶ˆæ¯
    messages = [{"role": "system", "content": system_prompt}]
    
    for msg in request.history:
        messages.append({"role": msg.get("role", "user"), "content": msg.get("content", "")})
    
    messages.append({"role": "user", "content": request.message})
    
    async def generate() -> AsyncGenerator[str, None]:
        # å…ˆå‘é€æ¥æºä¿¡æ¯
        if sources:
            yield f"data: {json.dumps({'sources': [s.model_dump() for s in sources]})}\n\n"
        
        try:
            async with httpx.AsyncClient(timeout=120.0) as client:
                async with client.stream(
                    "POST",
                    f"{config.base_url}/chat/completions",
                    headers={
                        "Content-Type": "application/json",
                        "Authorization": f"Bearer {config.api_key}"
                    },
                    json={
                        "model": config.model,
                        "messages": messages,
                        "stream": True
                    }
                ) as response:
                    if response.status_code != 200:
                        error_text = await response.aread()
                        yield f"data: {json.dumps({'error': error_text.decode()})}\n\n"
                        return
                    
                    async for line in response.aiter_lines():
                        if line.startswith("data: "):
                            data = line[6:]
                            if data == "[DONE]":
                                yield f"data: {json.dumps({'done': True})}\n\n"
                                break
                            try:
                                chunk = json.loads(data)
                                content = chunk.get("choices", [{}])[0].get("delta", {}).get("content", "")
                                if content:
                                    yield f"data: {json.dumps({'content': content})}\n\n"
                            except json.JSONDecodeError:
                                pass
                                
        except Exception as e:
            yield f"data: {json.dumps({'error': str(e)})}\n\n"
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        }
    )


# ==================== Agent Tool Calling API ====================

@router.get("/agent/tools")
async def get_agent_tools(
    category: Optional[str] = None,
    current_user: User = Depends(get_current_user),
):
    """è·å–å¯ç”¨çš„ Agent å·¥å…·åˆ—è¡¨"""
    categories = [category] if category else None
    tools = tool_registry.get_tools_info(categories=categories)
    return {
        "tools": tools,
        "categories": tool_registry.get_categories(),
    }


@router.get("/agent/tools/openai-format")
async def get_agent_tools_openai_format(
    category: Optional[str] = None,
    current_user: User = Depends(get_current_user),
):
    """è·å– OpenAI Function Calling æ ¼å¼çš„å·¥å…·å®šä¹‰"""
    categories = [category] if category else None
    return tool_registry.get_openai_tools(categories=categories)


@router.post("/agent/execute")
async def execute_agent_tool(
    tool_name: str,
    arguments: Dict[str, Any],
    current_user: User = Depends(get_current_user),
):
    """
    æ‰§è¡Œå•ä¸ª Agent å·¥å…·
    
    ç”¨äºæ‰‹åŠ¨æµ‹è¯•å·¥å…·æˆ–å‰ç«¯ç›´æ¥è°ƒç”¨å·¥å…·ã€‚
    """
    result = await tool_executor.execute(tool_name, arguments, require_confirmation=False)
    return result.model_dump()


# Agent å¢å¼ºç³»ç»Ÿæç¤ºè¯
AGENT_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ Web å®‰å…¨åˆ†æåŠ©æ‰‹ï¼Œå…·æœ‰ä½¿ç”¨å·¥å…·çš„èƒ½åŠ›ã€‚

## ä½ çš„èƒ½åŠ›
- åˆ†æ HTTP è¯·æ±‚/å“åº”ä¸­çš„å®‰å…¨é—®é¢˜
- è¯†åˆ«å¸¸è§æ¼æ´ï¼ˆSQLæ³¨å…¥ã€XSSã€CSRFã€SSRFç­‰ï¼‰
- ä½¿ç”¨å·¥å…·è¿›è¡Œç¼–ç /è§£ç ã€å“ˆå¸Œè®¡ç®—ã€ç½‘ç»œæŸ¥è¯¢ç­‰æ“ä½œ
- ç”Ÿæˆæµ‹è¯• payload
- æä¾›ä¿®å¤å»ºè®®

## å·¥å…·ä½¿ç”¨åŸåˆ™
- å½“éœ€è¦è¿›è¡Œç¼–ç ã€è§£ç ã€å“ˆå¸Œè®¡ç®—ç­‰æ“ä½œæ—¶ï¼Œä½¿ç”¨å¯¹åº”çš„å·¥å…·
- å½“éœ€è¦æŸ¥è¯¢åŸŸåã€IP ä¿¡æ¯æ—¶ï¼Œä½¿ç”¨ç½‘ç»œæŸ¥è¯¢å·¥å…·
- å·¥å…·æ‰§è¡Œç»“æœä¼šè¿”å›ç»™ä½ ï¼Œè¯·åŸºäºç»“æœç»§ç»­åˆ†æ

## å›å¤æ ¼å¼
å½“ä½ ä½¿ç”¨å·¥å…·æ—¶ï¼Œè¯·è¯´æ˜ä½ è¦åšä»€ä¹ˆä»¥åŠä¸ºä»€ä¹ˆã€‚
å½“ä½ å¾—åˆ°å·¥å…·ç»“æœåï¼Œè¯·å¯¹ç»“æœè¿›è¡Œåˆ†æå’Œè§£é‡Šã€‚

## é‡è¦åŸåˆ™
- åªåœ¨ç”¨æˆ·æœ‰æˆæƒçš„æƒ…å†µä¸‹è¿›è¡Œæµ‹è¯•å»ºè®®
- æä¾›å¯æ“ä½œçš„å…·ä½“å»ºè®®
- è§£é‡Šæ¸…æ¥šæ¼æ´åŸç†"""


from pydantic import BaseModel, Field


class AgentChatRequest(BaseModel):
    """Agent èŠå¤©è¯·æ±‚"""
    message: str = Field(..., description="ç”¨æˆ·æ¶ˆæ¯")
    history: List[Dict[str, Any]] = Field(default=[], description="å¯¹è¯å†å²")
    use_tools: bool = Field(default=True, description="æ˜¯å¦å¯ç”¨å·¥å…·è°ƒç”¨")
    tool_categories: Optional[List[str]] = Field(default=None, description="å¯ç”¨çš„å·¥å…·åˆ†ç±»")
    use_knowledge: bool = Field(default=False, description="æ˜¯å¦ä½¿ç”¨çŸ¥è¯†åº“")
    knowledge_sources: List[str] = Field(default=["note", "bookmark"], description="çŸ¥è¯†åº“æ¥æºç±»å‹")
    max_tool_iterations: int = Field(default=5, description="æœ€å¤§å·¥å…·è°ƒç”¨è½®æ¬¡")


@router.post("/agent/chat")
async def agent_chat_stream(
    request: AgentChatRequest,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Agent èŠå¤©æ¥å£ï¼ˆæµå¼ï¼Œæ”¯æŒ Tool Calling å’Œå®¡è®¡æ—¥å¿—ï¼‰
    
    æ”¯æŒ LLM è‡ªåŠ¨è°ƒç”¨å·¥å…·å®Œæˆä»»åŠ¡ï¼Œå·¥å…·æ‰§è¡Œç»“æœä¼šè‡ªåŠ¨åé¦ˆç»™ LLM ç»§ç»­å¤„ç†ã€‚
    æ‰€æœ‰æ“ä½œéƒ½ä¼šè¢«è®°å½•åˆ°å®¡è®¡æ—¥å¿—ä¸­ã€‚
    """
    config = await get_user_llm_config(current_user.id, db)
    
    # åˆ›å»ºå®¡è®¡æ—¥å¿—è®°å½•å™¨
    audit = AuditLogger(db, current_user.id)
    
    # å¼€å§‹ä¼šè¯
    session_id = await audit.start_session(
        message=request.message,
        model=config.model,
        provider=config.provider_id if hasattr(config, 'provider_id') else None,
        use_tools=request.use_tools,
        use_knowledge=request.use_knowledge,
    )
    
    # è®°å½•ç”¨æˆ·æ¶ˆæ¯
    await audit.log_user_message(request.message)
    
    # æ„å»ºç³»ç»Ÿæç¤ºè¯
    system_prompt = AGENT_SYSTEM_PROMPT
    sources: List[RAGSource] = []
    
    # RAG æ£€ç´¢
    if request.use_knowledge:
        await audit.log_rag_search(request.message, request.knowledge_sources)
        
        knowledge_items = await search_knowledge_base(
            user_id=current_user.id,
            query=request.message,
            source_types=request.knowledge_sources,
            limit=5,
            db=db,
        )
        
        await audit.log_rag_result(
            results=[{"title": item.title} for item in knowledge_items],
            sources=[item.source_type for item in knowledge_items],
        )
        
        if knowledge_items:
            knowledge_context, sources = build_knowledge_context(knowledge_items)
            system_prompt = f"""{AGENT_SYSTEM_PROMPT}

## ç”¨æˆ·çŸ¥è¯†åº“å‚è€ƒ
ä»¥ä¸‹æ˜¯ä»ç”¨æˆ·çŸ¥è¯†åº“ä¸­æ£€ç´¢åˆ°çš„ç›¸å…³å†…å®¹ï¼š

{knowledge_context}

---
è¯·åŸºäºä»¥ä¸ŠçŸ¥è¯†åº“å†…å®¹å’Œä½ çš„ä¸“ä¸šçŸ¥è¯†æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"""
    
    # æ„å»ºæ¶ˆæ¯
    messages = [{"role": "system", "content": system_prompt}]
    
    for msg in request.history:
        messages.append({"role": msg.get("role", "user"), "content": msg.get("content", "")})
    
    messages.append({"role": "user", "content": request.message})
    
    # è·å–å·¥å…·å®šä¹‰
    tools = []
    if request.use_tools:
        tools = tool_registry.get_openai_tools(categories=request.tool_categories)
    
    async def generate() -> AsyncGenerator[str, None]:
        nonlocal messages
        session_status = "completed"
        session_error = None
        
        try:
            # å…ˆå‘é€ä¼šè¯ ID
            yield f"data: {json.dumps({'session_id': session_id})}\n\n"
            
            # å‘é€çŸ¥è¯†åº“æ¥æº
            if sources:
                yield f"data: {json.dumps({'sources': [s.model_dump() for s in sources]})}\n\n"
            
            # å‘é€å¯ç”¨å·¥å…·ä¿¡æ¯
            if tools:
                tool_names = [t["function"]["name"] for t in tools]
                yield f"data: {json.dumps({'available_tools': tool_names})}\n\n"
            
            iteration = 0
            max_iterations = request.max_tool_iterations
            
            while iteration < max_iterations:
                iteration += 1
                
                try:
                    # è®°å½• LLM è¯·æ±‚
                    await audit.log_llm_request(messages, tools if tools else None)
                    
                    async with httpx.AsyncClient(timeout=120.0) as client:
                        # æ„å»ºè¯·æ±‚ä½“
                        request_body = {
                            "model": config.model,
                            "messages": messages,
                            "stream": True,
                        }
                        
                        # æ·»åŠ å·¥å…·ï¼ˆå¦‚æœæœ‰ï¼‰
                        if tools:
                            request_body["tools"] = tools
                            request_body["tool_choice"] = "auto"
                        
                        async with client.stream(
                            "POST",
                            f"{config.base_url}/chat/completions",
                            headers={
                                "Content-Type": "application/json",
                                "Authorization": f"Bearer {config.api_key}"
                            },
                            json=request_body
                        ) as response:
                            if response.status_code != 200:
                                error_text = await response.aread()
                                error_msg = error_text.decode()
                                await audit.log_llm_error(error_msg)
                                session_status = "error"
                                session_error = error_msg
                                yield f"data: {json.dumps({'error': error_msg})}\n\n"
                                return
                            
                            # æ”¶é›†å®Œæ•´å“åº”
                            full_content = ""
                            tool_calls_data: Dict[int, Dict] = {}
                            finish_reason = None
                            
                            async for line in response.aiter_lines():
                                if not line.startswith("data: "):
                                    continue
                                
                                data = line[6:]
                                if data == "[DONE]":
                                    break
                                
                                try:
                                    chunk = json.loads(data)
                                    choice = chunk.get("choices", [{}])[0]
                                    delta = choice.get("delta", {})
                                    finish_reason = choice.get("finish_reason") or finish_reason
                                    
                                    # å¤„ç†æ–‡æœ¬å†…å®¹
                                    content = delta.get("content", "")
                                    if content:
                                        full_content += content
                                        yield f"data: {json.dumps({'content': content})}\n\n"
                                    
                                    # å¤„ç†å·¥å…·è°ƒç”¨
                                    if "tool_calls" in delta:
                                        for tc in delta["tool_calls"]:
                                            idx = tc.get("index", 0)
                                            if idx not in tool_calls_data:
                                                tool_calls_data[idx] = {
                                                    "id": tc.get("id", ""),
                                                    "type": "function",
                                                    "function": {"name": "", "arguments": ""}
                                                }
                                            
                                            if tc.get("id"):
                                                tool_calls_data[idx]["id"] = tc["id"]
                                            
                                            func = tc.get("function", {})
                                            if func.get("name"):
                                                tool_calls_data[idx]["function"]["name"] = func["name"]
                                            if func.get("arguments"):
                                                tool_calls_data[idx]["function"]["arguments"] += func["arguments"]
                                    
                                except json.JSONDecodeError:
                                    pass
                            
                            # è®°å½• LLM å“åº”
                            has_tool_calls = finish_reason == "tool_calls" and bool(tool_calls_data)
                            await audit.log_llm_response(full_content, has_tool_calls=has_tool_calls)
                            
                            # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
                            if has_tool_calls:
                                tool_calls = list(tool_calls_data.values())
                                
                                # å°† assistant æ¶ˆæ¯ï¼ˆåŒ…å«å·¥å…·è°ƒç”¨ï¼‰æ·»åŠ åˆ°å†å²
                                assistant_message = {"role": "assistant", "content": full_content or None}
                                if tool_calls:
                                    assistant_message["tool_calls"] = tool_calls
                                messages.append(assistant_message)
                                
                                # æ‰§è¡Œå·¥å…·è°ƒç”¨
                                for tc in tool_calls:
                                    tool_name = tc["function"]["name"]
                                    tool_call_id = tc["id"]
                                    
                                    try:
                                        arguments = json.loads(tc["function"]["arguments"])
                                    except json.JSONDecodeError:
                                        arguments = {}
                                    
                                    # è®°å½•å·¥å…·è°ƒç”¨
                                    await audit.log_tool_call(tool_name, arguments, tool_call_id)
                                    
                                    # é€šçŸ¥å‰ç«¯å·¥å…·è°ƒç”¨å¼€å§‹
                                    yield f"data: {json.dumps({'tool_call': {'name': tool_name, 'arguments': arguments, 'status': 'executing'}})}\n\n"
                                    
                                    # æ‰§è¡Œå·¥å…·
                                    result = await tool_executor.execute(tool_name, arguments, require_confirmation=False)
                                    
                                    # è®°å½•å·¥å…·ç»“æœ
                                    await audit.log_tool_result(tool_name, result.model_dump(), result.success)
                                    
                                    # æ ¼å¼åŒ–ç»“æœ
                                    if result.success:
                                        result_content = json.dumps(result.data, ensure_ascii=False, indent=2)
                                    else:
                                        result_content = f"é”™è¯¯: {result.error}"
                                    
                                    # é€šçŸ¥å‰ç«¯å·¥å…·æ‰§è¡Œå®Œæˆ
                                    yield f"data: {json.dumps({'tool_call': {'name': tool_name, 'result': result.model_dump(), 'status': 'completed'}})}\n\n"
                                    
                                    # å°†å·¥å…·ç»“æœæ·»åŠ åˆ°æ¶ˆæ¯
                                    messages.append({
                                        "role": "tool",
                                        "tool_call_id": tool_call_id,
                                        "content": result_content,
                                    })
                                
                                # ç»§ç»­ä¸‹ä¸€è½®å¯¹è¯
                                continue
                            
                            # æ²¡æœ‰å·¥å…·è°ƒç”¨æˆ–æ­£å¸¸ç»“æŸï¼Œé€€å‡ºå¾ªç¯
                            yield f"data: {json.dumps({'done': True})}\n\n"
                            return
                            
                except Exception as e:
                    error_msg = str(e)
                    logger.exception("Agent chat error")
                    await audit.log_llm_error(error_msg)
                    session_status = "error"
                    session_error = error_msg
                    yield f"data: {json.dumps({'error': error_msg})}\n\n"
                    return
            
            # è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
            yield f"data: {json.dumps({'warning': 'è¾¾åˆ°æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•°é™åˆ¶', 'done': True})}\n\n"
            
        finally:
            # ç»“æŸä¼šè¯
            await audit.end_session(status=session_status, error=session_error)
            await db.commit()
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        }
    )


# ==================== å®¡è®¡æ—¥å¿— API ====================

@router.get("/agent/sessions")
async def list_agent_sessions(
    limit: int = 50,
    offset: int = 0,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
):
    """è·å–ç”¨æˆ·çš„ Agent ä¼šè¯åˆ—è¡¨"""
    sessions = await get_user_sessions(db, current_user.id, limit, offset)
    return {
        "sessions": [
            {
                "id": s.id,
                "initial_message": s.initial_message,
                "model": s.model,
                "provider": s.provider,
                "message_count": s.message_count,
                "tool_call_count": s.tool_call_count,
                "status": s.status,
                "started_at": s.started_at.isoformat() if s.started_at else None,
                "ended_at": s.ended_at.isoformat() if s.ended_at else None,
            }
            for s in sessions
        ],
        "total": len(sessions),
    }


@router.get("/agent/sessions/{session_id}")
async def get_agent_session(
    session_id: str,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
):
    """è·å–ä¼šè¯è¯¦æƒ…å’Œå®Œæ•´æ—¥å¿—"""
    session = await get_session_detail(db, session_id, current_user.id)
    if not session:
        raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")
    
    logs = await get_session_logs(db, session_id, current_user.id)
    
    return {
        "session": {
            "id": session.id,
            "initial_message": session.initial_message,
            "model": session.model,
            "provider": session.provider,
            "message_count": session.message_count,
            "tool_call_count": session.tool_call_count,
            "total_tokens": session.total_tokens,
            "use_tools": bool(session.use_tools),
            "use_knowledge": bool(session.use_knowledge),
            "status": session.status,
            "error_message": session.error_message,
            "started_at": session.started_at.isoformat() if session.started_at else None,
            "ended_at": session.ended_at.isoformat() if session.ended_at else None,
        },
        "logs": [
            {
                "id": log.id,
                "event_type": log.event_type,
                "event_order": log.event_order,
                "content": log.content,
                "extra_data": log.extra_data,
                "tool_name": log.tool_name,
                "tool_arguments": log.tool_arguments,
                "tool_result": log.tool_result,
                "duration_ms": log.duration_ms,
                "tokens_used": log.tokens_used,
                "success": bool(log.success),
                "error_message": log.error_message,
                "created_at": log.created_at.isoformat() if log.created_at else None,
            }
            for log in logs
        ],
    }


@router.get("/agent/stats")
async def get_agent_stats(
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
):
    """è·å–ç”¨æˆ·çš„ Agent ä½¿ç”¨ç»Ÿè®¡"""
    stats = await get_user_stats(db, current_user.id)
    return stats


@router.delete("/agent/sessions/{session_id}")
async def delete_agent_session(
    session_id: str,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
):
    """åˆ é™¤ä¼šè¯åŠå…¶æ—¥å¿—"""
    session = await get_session_detail(db, session_id, current_user.id)
    if not session:
        raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")
    
    await db.delete(session)
    await db.commit()
    
    return {"message": "ä¼šè¯å·²åˆ é™¤"}
