"""LLM é…ç½®å’ŒèŠå¤© API"""
from fastapi import APIRouter, Depends, HTTPException
from fastapi.responses import StreamingResponse
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, or_
import httpx
import json
import logging
from typing import AsyncGenerator, List, Optional, Any, Dict

logger = logging.getLogger(__name__)

from ...database import get_db
from ...config import settings
from ...models import User, UserLLMConfig, KnowledgeItem
from ...schemas.llm import (
    LLMConfigCreate, 
    LLMConfigResponse, 
    ChatRequest, 
    ChatResponse,
    LLM_PROVIDERS,
    LLM_PROVIDERS_MAP,
    LLMProvider
)
from ...schemas.knowledge import RAGChatRequest, RAGSource
from ..deps import get_current_user

# Agent Tool Calling
from ...agent import tool_registry, tool_executor

router = APIRouter(prefix="/llm", tags=["LLM"])


# ==================== HTTP å®¢æˆ·ç«¯è¿æ¥æ±  ====================

_llm_http_client: Optional[httpx.AsyncClient] = None


def get_llm_http_client() -> httpx.AsyncClient:
    """è·å– LLM HTTP å®¢æˆ·ç«¯ï¼ˆå•ä¾‹ï¼Œå¤ç”¨è¿æ¥æ± ï¼‰"""
    global _llm_http_client
    if _llm_http_client is None or _llm_http_client.is_closed:
        _llm_http_client = httpx.AsyncClient(
            timeout=120.0,
            limits=httpx.Limits(max_keepalive_connections=10, max_connections=50),
        )
    return _llm_http_client


async def close_llm_http_client():
    """å…³é—­ LLM HTTP å®¢æˆ·ç«¯ï¼ˆåº”ç”¨å…³é—­æ—¶è°ƒç”¨ï¼‰"""
    global _llm_http_client
    if _llm_http_client and not _llm_http_client.is_closed:
        await _llm_http_client.aclose()
        _llm_http_client = None
        logger.info("LLM HTTP å®¢æˆ·ç«¯å·²å…³é—­")


# ==================== LLM æä¾›å•† ====================

@router.get("/providers", response_model=list[LLMProvider])
async def get_llm_providers():
    """è·å–æ‰€æœ‰ LLM æä¾›å•†åˆ—è¡¨"""
    return LLM_PROVIDERS


@router.get("/default-config")
async def get_default_config():
    """è·å–ç³»ç»Ÿé»˜è®¤ LLM é…ç½®çŠ¶æ€"""
    default_config = get_default_llm_config()
    
    if not default_config:
        return {
            "available": False,
            "provider_id": None,
            "model": None,
            "models": [],
        }
    
    # è·å–æä¾›å•†æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨
    provider = LLM_PROVIDERS_MAP.get(default_config.provider_id)
    models = provider.models if provider else [default_config.model]
    
    return {
        "available": True,
        "provider_id": default_config.provider_id,
        "provider_name": provider.name if provider else default_config.provider_id,
        "model": default_config.model,
        "models": models,
    }


# ==================== ç”¨æˆ· LLM é…ç½® ====================

@router.get("/config", response_model=LLMConfigResponse)
async def get_llm_config(
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """è·å–å½“å‰ç”¨æˆ·çš„ LLM é…ç½®"""
    result = await db.execute(
        select(UserLLMConfig).where(UserLLMConfig.user_id == current_user.id)
    )
    config = result.scalar_one_or_none()
    
    if not config:
        raise HTTPException(status_code=404, detail="LLM é…ç½®æœªè®¾ç½®")
    
    return LLMConfigResponse(
        id=config.id,
        provider_id=config.provider_id,
        api_key_set=bool(config.api_key),
        base_url=config.base_url,
        model=config.model,
        use_system_default=bool(config.use_system_default),
        created_at=config.created_at,
        updated_at=config.updated_at
    )


@router.put("/config", response_model=LLMConfigResponse)
async def update_llm_config(
    config_data: LLMConfigCreate,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """æ›´æ–°å½“å‰ç”¨æˆ·çš„ LLM é…ç½®"""
    result = await db.execute(
        select(UserLLMConfig).where(UserLLMConfig.user_id == current_user.id)
    )
    config = result.scalar_one_or_none()
    
    base_url = config_data.base_url
    if not base_url and config_data.provider_id in LLM_PROVIDERS_MAP:
        base_url = LLM_PROVIDERS_MAP[config_data.provider_id].base_url
    
    if config:
        config.provider_id = config_data.provider_id
        if config_data.api_key is not None:
            config.api_key = config_data.api_key
        config.base_url = base_url
        config.model = config_data.model
        config.use_system_default = config_data.use_system_default
    else:
        config = UserLLMConfig(
            user_id=current_user.id,
            provider_id=config_data.provider_id,
            api_key=config_data.api_key,
            base_url=base_url,
            model=config_data.model,
            use_system_default=config_data.use_system_default
        )
        db.add(config)
    
    await db.flush()
    await db.refresh(config)
    
    return LLMConfigResponse(
        id=config.id,
        provider_id=config.provider_id,
        api_key_set=bool(config.api_key),
        base_url=config.base_url,
        model=config.model,
        use_system_default=bool(config.use_system_default),
        created_at=config.created_at,
        updated_at=config.updated_at
    )


@router.delete("/config")
async def delete_llm_config(
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """åˆ é™¤å½“å‰ç”¨æˆ·çš„ LLM é…ç½®"""
    result = await db.execute(
        select(UserLLMConfig).where(UserLLMConfig.user_id == current_user.id)
    )
    config = result.scalar_one_or_none()
    
    if config:
        await db.delete(config)
    
    return {"message": "é…ç½®å·²åˆ é™¤"}


# ==================== èŠå¤© API ====================

# åŸºç¡€ç³»ç»Ÿæç¤ºè¯
BASE_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ Web å®‰å…¨åˆ†æåŠ©æ‰‹ã€‚

## ä½ çš„èƒ½åŠ›
- åˆ†æ HTTP è¯·æ±‚/å“åº”ä¸­çš„å®‰å…¨é—®é¢˜
- è¯†åˆ«å¸¸è§æ¼æ´ï¼ˆSQLæ³¨å…¥ã€XSSã€CSRFã€SSRFç­‰ï¼‰
- ç”Ÿæˆæµ‹è¯• payload
- æä¾›ä¿®å¤å»ºè®®

## å›å¤æ ¼å¼
å½“ç”¨æˆ·æä¾› HTTP è¯·æ±‚æˆ–è¯¢é—®å®‰å…¨é—®é¢˜æ—¶ï¼Œè¯·æŒ‰ä»¥ä¸‹ç»“æ„å›å¤ï¼š

### 1. ğŸ” åˆ†æ
ç®€è¦åˆ†æè¯·æ±‚ç»“æ„å’Œæ½œåœ¨é£é™©ç‚¹

### 2. ğŸ¯ æ½œåœ¨æ¼æ´
åˆ—å‡ºå¯èƒ½å­˜åœ¨çš„æ¼æ´ç±»å‹å’Œé£é™©ç­‰çº§

### 3. ğŸ§ª æµ‹è¯•å»ºè®®
æä¾›å…·ä½“çš„æµ‹è¯• payloadï¼ˆå¯ç›´æ¥å¤åˆ¶ä½¿ç”¨ï¼‰

### 4. ğŸ›¡ï¸ ä¿®å¤å»ºè®®
å¦‚æœå‘ç°é—®é¢˜ï¼Œæä¾›ä¿®å¤æ–¹æ¡ˆ

## é‡è¦åŸåˆ™
- åªåœ¨ç”¨æˆ·æœ‰æˆæƒçš„æƒ…å†µä¸‹è¿›è¡Œæµ‹è¯•å»ºè®®
- æä¾›å¯æ“ä½œçš„å…·ä½“å»ºè®®
- è§£é‡Šæ¸…æ¥šæ¼æ´åŸç†"""

# RAG å¢å¼ºç³»ç»Ÿæç¤ºè¯æ¨¡æ¿
RAG_SYSTEM_PROMPT_TEMPLATE = """{base_prompt}

## ç”¨æˆ·çŸ¥è¯†åº“å‚è€ƒ
ä»¥ä¸‹æ˜¯ä»ç”¨æˆ·çŸ¥è¯†åº“ä¸­æ£€ç´¢åˆ°çš„ç›¸å…³å†…å®¹ï¼Œè¯·ä¼˜å…ˆå‚è€ƒè¿™äº›å†…å®¹æ¥å›ç­”é—®é¢˜ï¼š

{knowledge_context}

---
è¯·åŸºäºä»¥ä¸ŠçŸ¥è¯†åº“å†…å®¹å’Œä½ çš„ä¸“ä¸šçŸ¥è¯†æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœçŸ¥è¯†åº“ä¸­æœ‰ç›¸å…³å†…å®¹ï¼Œè¯·åœ¨å›ç­”ä¸­å¼•ç”¨ã€‚"""


class DefaultLLMConfig:
    """é»˜è®¤ LLM é…ç½®ï¼ˆç”¨äºç³»ç»Ÿé»˜è®¤é…ç½®ï¼‰"""
    def __init__(self, provider_id: str, api_key: str, base_url: str, model: str):
        self.provider_id = provider_id
        self.api_key = api_key
        self.base_url = base_url
        self.model = model


def get_default_llm_config() -> Optional[DefaultLLMConfig]:
    """è·å–ç³»ç»Ÿé»˜è®¤ LLM é…ç½®"""
    if not settings.DEFAULT_LLM_API_KEY:
        return None
    
    provider_id = settings.DEFAULT_LLM_PROVIDER or "deepseek"
    
    # è·å– base_urlï¼šä¼˜å…ˆä½¿ç”¨é…ç½®çš„ï¼Œå¦åˆ™ä½¿ç”¨æä¾›å•†é»˜è®¤çš„
    base_url = settings.DEFAULT_LLM_BASE_URL
    if not base_url and provider_id in LLM_PROVIDERS_MAP:
        base_url = LLM_PROVIDERS_MAP[provider_id].base_url
    
    # è·å– modelï¼šä¼˜å…ˆä½¿ç”¨é…ç½®çš„ï¼Œå¦åˆ™ä½¿ç”¨æä¾›å•†é»˜è®¤çš„
    model = settings.DEFAULT_LLM_MODEL
    if not model and provider_id in LLM_PROVIDERS_MAP:
        model = LLM_PROVIDERS_MAP[provider_id].default_model
    
    if not base_url or not model:
        return None
    
    return DefaultLLMConfig(
        provider_id=provider_id,
        api_key=settings.DEFAULT_LLM_API_KEY,
        base_url=base_url,
        model=model
    )


async def get_user_llm_config(user_id: str, db: AsyncSession):
    """è·å–ç”¨æˆ·çš„ LLM é…ç½®ï¼Œå¦‚æœç”¨æˆ·æœªé…ç½®åˆ™å°è¯•ä½¿ç”¨ç³»ç»Ÿé»˜è®¤é…ç½®"""
    result = await db.execute(
        select(UserLLMConfig).where(UserLLMConfig.user_id == user_id)
    )
    config = result.scalar_one_or_none()
    
    # å¦‚æœç”¨æˆ·é€‰æ‹©ä½¿ç”¨ç³»ç»Ÿé»˜è®¤é…ç½®
    if config and config.use_system_default:
        default_config = get_default_llm_config()
        if default_config:
            # ä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„æ¨¡å‹ï¼Œä½†ç”¨ç³»ç»Ÿé»˜è®¤çš„ API Key
            default_config.model = config.model or default_config.model
            logger.info(f"ç”¨æˆ· {user_id} ä½¿ç”¨ç³»ç»Ÿé»˜è®¤ LLM é…ç½® (provider={default_config.provider_id}, model={default_config.model})")
            return default_config
        else:
            raise HTTPException(
                status_code=400,
                detail="ç³»ç»Ÿé»˜è®¤é…ç½®ä¸å¯ç”¨ï¼Œè¯·è”ç³»ç®¡ç†å‘˜æˆ–ä½¿ç”¨è‡ªå·±çš„ API Key"
            )
    
    # å¦‚æœç”¨æˆ·æœ‰é…ç½®ä¸”æœ‰ API Keyï¼ˆæˆ–è€…æ˜¯ Ollamaï¼‰ï¼Œä½¿ç”¨ç”¨æˆ·é…ç½®
    if config and (config.api_key or config.provider_id == "ollama"):
        return config
    
    # å°è¯•ä½¿ç”¨ç³»ç»Ÿé»˜è®¤é…ç½®ï¼ˆç”¨æˆ·æœªé…ç½®æ—¶çš„ fallbackï¼‰
    default_config = get_default_llm_config()
    if default_config:
        logger.info(f"ç”¨æˆ· {user_id} ä½¿ç”¨ç³»ç»Ÿé»˜è®¤ LLM é…ç½® (provider={default_config.provider_id})")
        return default_config
    
    # éƒ½æ²¡æœ‰é…ç½®ï¼ŒæŠ¥é”™
    if not config:
        raise HTTPException(
            status_code=400, 
            detail="è¯·å…ˆåœ¨è®¾ç½®ä¸­é…ç½® LLM API Key"
        )
    
    raise HTTPException(
        status_code=400,
        detail="è¯·å…ˆé…ç½® API Key"
    )


async def search_knowledge_base(
    user_id: str,
    query: str,
    source_types: List[str],
    limit: int,
    db: AsyncSession
) -> List[KnowledgeItem]:
    """æœç´¢çŸ¥è¯†åº“"""
    if not query.strip():
        return []
    
    # å…ˆç»Ÿè®¡ç”¨æˆ·çŸ¥è¯†åº“æ€»æ•°ï¼ˆè°ƒè¯•ç”¨ï¼‰
    total_query = select(KnowledgeItem).where(
        KnowledgeItem.user_id == user_id,
        KnowledgeItem.is_enabled == True,
    )
    if source_types:
        total_query = total_query.where(KnowledgeItem.source_type.in_(source_types))
    total_result = await db.execute(total_query)
    all_items = list(total_result.scalars().all())
    print(f"ğŸ“Š [RAG] ç”¨æˆ·çŸ¥è¯†åº“æ€»æ•°: {len(all_items)} æ¡ (æ¥æºç±»å‹: {source_types})")
    if all_items:
        print(f"ğŸ“Š [RAG] çŸ¥è¯†åº“æ ‡é¢˜: {[item.title[:30] for item in all_items[:5]]}...")
    
    # æ„å»ºæŸ¥è¯¢
    db_query = select(KnowledgeItem).where(
        KnowledgeItem.user_id == user_id,
        KnowledgeItem.is_enabled == True,
    )
    
    # ç­›é€‰æ¥æºç±»å‹
    if source_types:
        db_query = db_query.where(KnowledgeItem.source_type.in_(source_types))
    
    # ä½¿ç”¨ jieba è¿›è¡Œä¸­æ–‡åˆ†è¯
    import jieba
    import jieba.analyse
    
    # ä½¿ç”¨ TF-IDF æå–å…³é”®è¯ï¼ˆæ›´æ™ºèƒ½çš„è¯­ä¹‰åˆ†å‰²ï¼‰
    keywords = jieba.analyse.extract_tags(query, topK=8, withWeight=False)
    
    # è¡¥å……ï¼šæå–è‹±æ–‡å•è¯ï¼ˆjieba å¯¹è‹±æ–‡å¤„ç†è¾ƒå¼±ï¼‰
    import re
    eng_words = re.findall(r'[a-zA-Z]{2,}', query)
    keywords = list(dict.fromkeys(eng_words + keywords))[:10]
    
    logger.debug(f"[RAG] åˆ†è¯å…³é”®è¯: {keywords}")
    
    if keywords:
        # æ„å»º OR æ¡ä»¶ï¼šæ ‡é¢˜æˆ–å†…å®¹åŒ…å«ä»»æ„å…³é”®è¯
        conditions = []
        for keyword in keywords[:5]:  # é™åˆ¶å…³é”®è¯æ•°é‡
            pattern = f"%{keyword}%"
            conditions.append(KnowledgeItem.title.ilike(pattern))
            conditions.append(KnowledgeItem.content.ilike(pattern))
        
        db_query = db_query.where(or_(*conditions))
    
    db_query = db_query.limit(limit)
    
    result = await db.execute(db_query)
    return list(result.scalars().all())


def build_knowledge_context(items: List[KnowledgeItem]) -> tuple[str, List[RAGSource]]:
    """æ„å»ºçŸ¥è¯†åº“ä¸Šä¸‹æ–‡å’Œæ¥æºåˆ—è¡¨ï¼ˆä¼˜å…ˆä½¿ç”¨æ‘˜è¦ï¼‰"""
    if not items:
        return "", []
    
    context_parts = []
    sources = []
    
    for i, item in enumerate(items, 1):
        # æ¥æºç±»å‹æ ‡è®°
        type_emoji = {"note": "ğŸ“", "bookmark": "ğŸ”—", "file": "ğŸ“„"}.get(item.source_type, "ğŸ“‹")
        
        # ä¼˜å…ˆä½¿ç”¨æ‘˜è¦ï¼Œæ²¡æœ‰æ‘˜è¦åˆ™ä½¿ç”¨å†…å®¹é¢„è§ˆ
        if item.summary:
            # æœ‰æ‘˜è¦æ—¶ï¼Œä½¿ç”¨æ‘˜è¦ä½œä¸ºä¸»è¦å†…å®¹
            content_text = f"**æ‘˜è¦:** {item.summary}"
            # å¦‚æœå†…å®¹ä¸å¤ªé•¿ï¼Œä¹Ÿé™„å¸¦éƒ¨åˆ†å†…å®¹
            if item.content and len(item.content) <= 500:
                content_text += f"\n\n**è¯¦æƒ…:** {item.content}"
            elif item.content:
                content_text += f"\n\n**è¯¦æƒ…é¢„è§ˆ:** {item.content[:300]}..."
        else:
            # æ²¡æœ‰æ‘˜è¦æ—¶ï¼Œä½¿ç”¨å†…å®¹é¢„è§ˆ
            content_text = item.content[:1000] if item.content else item.title
        
        context_parts.append(f"""### {type_emoji} [{i}] {item.title}
{content_text}
""")
        
        # æ„å»ºæ¥æºï¼ˆsnippet ä¼˜å…ˆä½¿ç”¨æ‘˜è¦ï¼‰
        snippet = item.summary if item.summary else (item.content[:200] if item.content else "")
        sources.append(RAGSource(
            source_type=item.source_type,
            source_id=item.source_id,
            title=item.title,
            snippet=snippet,
            url=item.url,
        ))
    
    return "\n".join(context_parts), sources


@router.post("/chat", response_model=ChatResponse)
async def chat(
    request: ChatRequest,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """èŠå¤©æ¥å£ï¼ˆéæµå¼ï¼‰"""
    config = await get_user_llm_config(current_user.id, db)
    
    # RAG æ£€ç´¢
    sources = []
    system_prompt = BASE_SYSTEM_PROMPT
    
    if request.use_rag:
        # æœç´¢çŸ¥è¯†åº“
        knowledge_items = await search_knowledge_base(
            user_id=current_user.id,
            query=request.message,
            source_types=["note", "bookmark", "file"],
            limit=5,
            db=db,
        )
        
        if knowledge_items:
            knowledge_context, sources = build_knowledge_context(knowledge_items)
            system_prompt = RAG_SYSTEM_PROMPT_TEMPLATE.format(
                base_prompt=BASE_SYSTEM_PROMPT,
                knowledge_context=knowledge_context,
            )
    
    # æ„å»ºæ¶ˆæ¯
    messages = [{"role": "system", "content": system_prompt}]
    
    for msg in request.history:
        messages.append({"role": msg.role, "content": msg.content})
    
    messages.append({"role": "user", "content": request.message})
    
    # è°ƒç”¨ LLMï¼ˆä½¿ç”¨è¿æ¥æ± ï¼‰
    try:
        client = get_llm_http_client()
        response = await client.post(
            f"{config.base_url}/chat/completions",
            headers={
                "Content-Type": "application/json",
                "Authorization": f"Bearer {config.api_key}"
            },
            json={
                "model": config.model,
                "messages": messages,
                "stream": False
            }
        )
        
        if response.status_code != 200:
            error_detail = response.text
            try:
                error_json = response.json()
                error_detail = error_json.get("error", {}).get("message", error_detail)
            except:
                pass
            raise HTTPException(status_code=response.status_code, detail=error_detail)
        
        result = response.json()
        content = result["choices"][0]["message"]["content"]
        
        return ChatResponse(
            content=content, 
            sources=[s.model_dump() for s in sources]
        )
            
    except httpx.TimeoutException:
        raise HTTPException(status_code=504, detail="LLM è¯·æ±‚è¶…æ—¶")
    except httpx.RequestError as e:
        raise HTTPException(status_code=502, detail=f"LLM è¯·æ±‚å¤±è´¥: {str(e)}")


@router.post("/chat/stream")
async def chat_stream(
    request: RAGChatRequest,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """èŠå¤©æ¥å£ï¼ˆæµå¼ï¼Œæ”¯æŒ RAGï¼‰"""
    import time
    start_time = time.time()
    
    logger.info(f"ğŸ’¬ [Chat] å¼€å§‹å¤„ç†: user={current_user.username}, message={request.message[:50]}..., use_knowledge={request.use_knowledge}")
    
    config = await get_user_llm_config(current_user.id, db)
    logger.debug(f"ğŸ’¬ [Chat] LLMé…ç½®: model={config.model}")
    
    # RAG æ£€ç´¢
    sources: List[RAGSource] = []
    system_prompt = BASE_SYSTEM_PROMPT
    
    print(f"ğŸ” [Chat] ç”¨æˆ·={current_user.username}, çŸ¥è¯†åº“={request.use_knowledge}, æ¥æºç±»å‹={request.knowledge_sources}")
    
    if request.use_knowledge:
        # æœç´¢çŸ¥è¯†åº“
        knowledge_items = await search_knowledge_base(
            user_id=current_user.id,
            query=request.message,
            source_types=request.knowledge_sources,
            limit=request.max_results,
            db=db,
        )
        
        print(f"ğŸ“š [Chat] çŸ¥è¯†åº“æ£€ç´¢ç»“æœ: {len(knowledge_items)} æ¡")
        
        if knowledge_items:
            knowledge_context, sources = build_knowledge_context(knowledge_items)
            system_prompt = RAG_SYSTEM_PROMPT_TEMPLATE.format(
                base_prompt=BASE_SYSTEM_PROMPT,
                knowledge_context=knowledge_context,
            )
            print(f"ğŸ“– [Chat] RAG æ¥æº: {[s.title for s in sources]}")
        else:
            print("âš ï¸ [Chat] çŸ¥è¯†åº“æœªæ£€ç´¢åˆ°ç›¸å…³å†…å®¹")
    
    # æ„å»ºæ¶ˆæ¯
    messages = [{"role": "system", "content": system_prompt}]
    
    for msg in request.history:
        messages.append({"role": msg.get("role", "user"), "content": msg.get("content", "")})
    
    messages.append({"role": "user", "content": request.message})
    
    async def generate() -> AsyncGenerator[str, None]:
        # å…ˆå‘é€æ¥æºä¿¡æ¯
        if sources:
            yield f"data: {json.dumps({'sources': [s.model_dump() for s in sources]})}\n\n"
        
        try:
            client = get_llm_http_client()
            async with client.stream(
                "POST",
                f"{config.base_url}/chat/completions",
                headers={
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {config.api_key}"
                },
                json={
                    "model": config.model,
                    "messages": messages,
                    "stream": True
                }
            ) as response:
                if response.status_code != 200:
                    error_text = await response.aread()
                    yield f"data: {json.dumps({'error': error_text.decode()})}\n\n"
                    return
                
                async for line in response.aiter_lines():
                    if line.startswith("data: "):
                        data = line[6:]
                        if data == "[DONE]":
                            yield f"data: {json.dumps({'done': True})}\n\n"
                            break
                        try:
                            chunk = json.loads(data)
                            content = chunk.get("choices", [{}])[0].get("delta", {}).get("content", "")
                            if content:
                                yield f"data: {json.dumps({'content': content})}\n\n"
                        except json.JSONDecodeError:
                            pass
                                
        except Exception as e:
            yield f"data: {json.dumps({'error': str(e)})}\n\n"
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache, no-store, must-revalidate",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",  # ç¦ç”¨ Nginx ç¼“å†²
            "Transfer-Encoding": "chunked",
        }
    )


# ==================== Agent Tool Calling API ====================

@router.get("/agent/tools")
async def get_agent_tools(
    category: Optional[str] = None,
    current_user: User = Depends(get_current_user),
):
    """è·å–å¯ç”¨çš„ Agent å·¥å…·åˆ—è¡¨"""
    categories = [category] if category else None
    tools = tool_registry.get_tools_info(categories=categories)
    return {
        "tools": tools,
        "categories": tool_registry.get_categories(),
    }


@router.get("/agent/tools/openai-format")
async def get_agent_tools_openai_format(
    category: Optional[str] = None,
    current_user: User = Depends(get_current_user),
):
    """è·å– OpenAI Function Calling æ ¼å¼çš„å·¥å…·å®šä¹‰"""
    categories = [category] if category else None
    return tool_registry.get_openai_tools(categories=categories)


@router.post("/agent/execute")
async def execute_agent_tool(
    tool_name: str,
    arguments: Dict[str, Any],
    current_user: User = Depends(get_current_user),
):
    """
    æ‰§è¡Œå•ä¸ª Agent å·¥å…·
    
    ç”¨äºæ‰‹åŠ¨æµ‹è¯•å·¥å…·æˆ–å‰ç«¯ç›´æ¥è°ƒç”¨å·¥å…·ã€‚
    """
    result = await tool_executor.execute(tool_name, arguments, require_confirmation=False)
    return result.model_dump()


# Agent å¢å¼ºç³»ç»Ÿæç¤ºè¯
AGENT_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ Web å®‰å…¨åˆ†æåŠ©æ‰‹ï¼Œå…·æœ‰ä½¿ç”¨å·¥å…·çš„èƒ½åŠ›ã€‚

## ä½ çš„èƒ½åŠ›
- åˆ†æ HTTP è¯·æ±‚/å“åº”ä¸­çš„å®‰å…¨é—®é¢˜
- è¯†åˆ«å¸¸è§æ¼æ´ï¼ˆSQLæ³¨å…¥ã€XSSã€CSRFã€SSRFç­‰ï¼‰
- ä½¿ç”¨å·¥å…·è¿›è¡Œç¼–ç /è§£ç ã€å“ˆå¸Œè®¡ç®—ã€ç½‘ç»œæŸ¥è¯¢ç­‰æ“ä½œ
- ç”Ÿæˆæµ‹è¯• payload
- æä¾›ä¿®å¤å»ºè®®

## å·¥å…·ä½¿ç”¨åŸåˆ™
- å½“éœ€è¦è¿›è¡Œç¼–ç ã€è§£ç ã€å“ˆå¸Œè®¡ç®—ç­‰æ“ä½œæ—¶ï¼Œä½¿ç”¨å¯¹åº”çš„å·¥å…·
- å½“éœ€è¦æŸ¥è¯¢åŸŸåã€IP ä¿¡æ¯æ—¶ï¼Œä½¿ç”¨ç½‘ç»œæŸ¥è¯¢å·¥å…·
- å·¥å…·æ‰§è¡Œç»“æœä¼šè¿”å›ç»™ä½ ï¼Œè¯·åŸºäºç»“æœç»§ç»­åˆ†æ

## å›å¤æ ¼å¼
å½“ä½ ä½¿ç”¨å·¥å…·æ—¶ï¼Œè¯·è¯´æ˜ä½ è¦åšä»€ä¹ˆä»¥åŠä¸ºä»€ä¹ˆã€‚
å½“ä½ å¾—åˆ°å·¥å…·ç»“æœåï¼Œè¯·å¯¹ç»“æœè¿›è¡Œåˆ†æå’Œè§£é‡Šã€‚

## é‡è¦åŸåˆ™
- åªåœ¨ç”¨æˆ·æœ‰æˆæƒçš„æƒ…å†µä¸‹è¿›è¡Œæµ‹è¯•å»ºè®®
- æä¾›å¯æ“ä½œçš„å…·ä½“å»ºè®®
- è§£é‡Šæ¸…æ¥šæ¼æ´åŸç†"""


from pydantic import BaseModel, Field


class AgentChatRequest(BaseModel):
    """Agent èŠå¤©è¯·æ±‚"""
    message: str = Field(..., description="ç”¨æˆ·æ¶ˆæ¯")
    history: List[Dict[str, Any]] = Field(default=[], description="å¯¹è¯å†å²")
    use_tools: bool = Field(default=True, description="æ˜¯å¦å¯ç”¨å·¥å…·è°ƒç”¨")
    tool_categories: Optional[List[str]] = Field(default=None, description="å¯ç”¨çš„å·¥å…·åˆ†ç±»")
    use_knowledge: bool = Field(default=False, description="æ˜¯å¦ä½¿ç”¨çŸ¥è¯†åº“")
    knowledge_sources: List[str] = Field(default=["note", "bookmark"], description="çŸ¥è¯†åº“æ¥æºç±»å‹")
    max_tool_iterations: int = Field(default=5, description="æœ€å¤§å·¥å…·è°ƒç”¨è½®æ¬¡")


@router.post("/agent/chat")
async def agent_chat_stream(
    request: AgentChatRequest,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Agent èŠå¤©æ¥å£ï¼ˆæµå¼ï¼Œæ”¯æŒ Tool Callingï¼‰
    
    æ”¯æŒ LLM è‡ªåŠ¨è°ƒç”¨å·¥å…·å®Œæˆä»»åŠ¡ï¼Œå·¥å…·æ‰§è¡Œç»“æœä¼šè‡ªåŠ¨åé¦ˆç»™ LLM ç»§ç»­å¤„ç†ã€‚
    """
    logger.info(f"ğŸ¤– [AgentChat] å¼€å§‹å¤„ç†: user={current_user.username}, message={request.message[:50]}..., use_tools={request.use_tools}")
    
    config = await get_user_llm_config(current_user.id, db)
    logger.debug(f"ğŸ¤– [AgentChat] LLMé…ç½®: model={config.model}")
    
    # æ„å»ºç³»ç»Ÿæç¤ºè¯
    system_prompt = AGENT_SYSTEM_PROMPT
    sources: List[RAGSource] = []
    
    # RAG æ£€ç´¢
    if request.use_knowledge:
        knowledge_items = await search_knowledge_base(
            user_id=current_user.id,
            query=request.message,
            source_types=request.knowledge_sources,
            limit=5,
            db=db,
        )
        
        if knowledge_items:
            knowledge_context, sources = build_knowledge_context(knowledge_items)
            system_prompt = f"""{AGENT_SYSTEM_PROMPT}

## ç”¨æˆ·çŸ¥è¯†åº“å‚è€ƒ
ä»¥ä¸‹æ˜¯ä»ç”¨æˆ·çŸ¥è¯†åº“ä¸­æ£€ç´¢åˆ°çš„ç›¸å…³å†…å®¹ï¼š

{knowledge_context}

---
è¯·åŸºäºä»¥ä¸ŠçŸ¥è¯†åº“å†…å®¹å’Œä½ çš„ä¸“ä¸šçŸ¥è¯†æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"""
    
    # æ„å»ºæ¶ˆæ¯
    messages = [{"role": "system", "content": system_prompt}]
    
    for msg in request.history:
        messages.append({"role": msg.get("role", "user"), "content": msg.get("content", "")})
    
    messages.append({"role": "user", "content": request.message})
    
    # è·å–å·¥å…·å®šä¹‰
    tools = []
    if request.use_tools:
        tools = tool_registry.get_openai_tools(categories=request.tool_categories)
    
    async def generate() -> AsyncGenerator[str, None]:
        nonlocal messages
        
        try:
            
            # å‘é€çŸ¥è¯†åº“æ¥æº
            if sources:
                yield f"data: {json.dumps({'sources': [s.model_dump() for s in sources]})}\n\n"
            
            # å‘é€å¯ç”¨å·¥å…·ä¿¡æ¯
            if tools:
                tool_names = [t["function"]["name"] for t in tools]
                yield f"data: {json.dumps({'available_tools': tool_names})}\n\n"
            
            iteration = 0
            max_iterations = request.max_tool_iterations
            
            while iteration < max_iterations:
                iteration += 1
                
                try:
                    client = get_llm_http_client()
                    # æ„å»ºè¯·æ±‚ä½“
                    request_body = {
                        "model": config.model,
                        "messages": messages,
                        "stream": True,
                    }
                    
                    # æ·»åŠ å·¥å…·ï¼ˆå¦‚æœæœ‰ï¼‰
                    if tools:
                        request_body["tools"] = tools
                        request_body["tool_choice"] = "auto"
                    
                    async with client.stream(
                        "POST",
                        f"{config.base_url}/chat/completions",
                        headers={
                            "Content-Type": "application/json",
                            "Authorization": f"Bearer {config.api_key}"
                        },
                        json=request_body
                    ) as response:
                            if response.status_code != 200:
                                error_text = await response.aread()
                                error_msg = error_text.decode()
                                yield f"data: {json.dumps({'error': error_msg})}\n\n"
                                return
                            
                            # æ”¶é›†å®Œæ•´å“åº”
                            full_content = ""
                            tool_calls_data: Dict[int, Dict] = {}
                            finish_reason = None
                            
                            async for line in response.aiter_lines():
                                if not line.startswith("data: "):
                                    continue
                                
                                data = line[6:]
                                if data == "[DONE]":
                                    break
                                
                                try:
                                    chunk = json.loads(data)
                                    choice = chunk.get("choices", [{}])[0]
                                    delta = choice.get("delta", {})
                                    finish_reason = choice.get("finish_reason") or finish_reason
                                    
                                    # å¤„ç†æ–‡æœ¬å†…å®¹
                                    content = delta.get("content", "")
                                    if content:
                                        full_content += content
                                        yield f"data: {json.dumps({'content': content})}\n\n"
                                    
                                    # å¤„ç†å·¥å…·è°ƒç”¨
                                    if "tool_calls" in delta:
                                        for tc in delta["tool_calls"]:
                                            idx = tc.get("index", 0)
                                            if idx not in tool_calls_data:
                                                tool_calls_data[idx] = {
                                                    "id": tc.get("id", ""),
                                                    "type": "function",
                                                    "function": {"name": "", "arguments": ""}
                                                }
                                            
                                            if tc.get("id"):
                                                tool_calls_data[idx]["id"] = tc["id"]
                                            
                                            func = tc.get("function", {})
                                            if func.get("name"):
                                                tool_calls_data[idx]["function"]["name"] = func["name"]
                                            if func.get("arguments"):
                                                tool_calls_data[idx]["function"]["arguments"] += func["arguments"]
                                    
                                except json.JSONDecodeError:
                                    pass
                            
                            # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
                            has_tool_calls = finish_reason == "tool_calls" and bool(tool_calls_data)
                            if has_tool_calls:
                                tool_calls = list(tool_calls_data.values())
                                
                                # å°† assistant æ¶ˆæ¯ï¼ˆåŒ…å«å·¥å…·è°ƒç”¨ï¼‰æ·»åŠ åˆ°å†å²
                                assistant_message = {"role": "assistant", "content": full_content or None}
                                if tool_calls:
                                    assistant_message["tool_calls"] = tool_calls
                                messages.append(assistant_message)
                                
                                # æ‰§è¡Œå·¥å…·è°ƒç”¨
                                for tc in tool_calls:
                                    tool_name = tc["function"]["name"]
                                    tool_call_id = tc["id"]
                                    
                                    try:
                                        arguments = json.loads(tc["function"]["arguments"])
                                    except json.JSONDecodeError:
                                        arguments = {}
                                    
                                    # é€šçŸ¥å‰ç«¯å·¥å…·è°ƒç”¨å¼€å§‹
                                    yield f"data: {json.dumps({'tool_call': {'name': tool_name, 'arguments': arguments, 'status': 'executing'}})}\n\n"
                                    
                                    # æ‰§è¡Œå·¥å…·
                                    result = await tool_executor.execute(tool_name, arguments, require_confirmation=False)
                                    
                                    # æ ¼å¼åŒ–ç»“æœ
                                    if result.success:
                                        result_content = json.dumps(result.data, ensure_ascii=False, indent=2)
                                    else:
                                        result_content = f"é”™è¯¯: {result.error}"
                                    
                                    # é€šçŸ¥å‰ç«¯å·¥å…·æ‰§è¡Œå®Œæˆ
                                    yield f"data: {json.dumps({'tool_call': {'name': tool_name, 'result': result.model_dump(), 'status': 'completed'}})}\n\n"
                                    
                                    # å°†å·¥å…·ç»“æœæ·»åŠ åˆ°æ¶ˆæ¯
                                    messages.append({
                                        "role": "tool",
                                        "tool_call_id": tool_call_id,
                                        "content": result_content,
                                    })
                                
                                # ç»§ç»­ä¸‹ä¸€è½®å¯¹è¯
                                continue
                            
                            # æ²¡æœ‰å·¥å…·è°ƒç”¨æˆ–æ­£å¸¸ç»“æŸï¼Œé€€å‡ºå¾ªç¯
                            yield f"data: {json.dumps({'done': True})}\n\n"
                            return
                            
                except Exception as e:
                    error_msg = str(e)
                    logger.exception("Agent chat error")
                    yield f"data: {json.dumps({'error': error_msg})}\n\n"
                    return
            
            # è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
            yield f"data: {json.dumps({'warning': 'è¾¾åˆ°æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•°é™åˆ¶', 'done': True})}\n\n"
            
        except Exception as e:
            logger.exception("Agent generate error")
            yield f"data: {json.dumps({'error': str(e)})}\n\n"
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache, no-store, must-revalidate",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",  # ç¦ç”¨ Nginx ç¼“å†²
            "Transfer-Encoding": "chunked",
        }
    )


# ==================== åŒ LLM æ¶æ„ API (çœ Token æ¨¡å¼) ====================

from ...agent.dual_llm import DualLLMAgent, LLMConfig, AgentMode, DualLLMResult, get_shared_agent
from ...agent.intent import IntentCategory


class UserContext(BaseModel):
    """ç”¨æˆ·ä¸Šä¸‹æ–‡ä¿¡æ¯"""
    location: Optional[str] = Field(default=None, description="ç”¨æˆ·ä½ç½®ï¼ˆåŸå¸‚åï¼‰")
    timezone: Optional[str] = Field(default=None, description="ç”¨æˆ·æ—¶åŒº")
    language: Optional[str] = Field(default="zh-CN", description="ç”¨æˆ·è¯­è¨€")


class FastChatRequest(BaseModel):
    """å¿«é€ŸèŠå¤©è¯·æ±‚ï¼ˆåŒ LLM æ¨¡å¼ï¼‰"""
    message: str = Field(..., description="ç”¨æˆ·æ¶ˆæ¯")
    mode: str = Field(default="auto", description="æ¨¡å¼: auto/fast/full")
    skip_summary: bool = Field(default=False, description="è·³è¿‡ Summary LLM")
    context: Optional[UserContext] = Field(default=None, description="ç”¨æˆ·ä¸Šä¸‹æ–‡ä¿¡æ¯")


class FastChatResponse(BaseModel):
    """å¿«é€ŸèŠå¤©å“åº”"""
    content: str = Field(..., description="å›å¤å†…å®¹")
    mode_used: str = Field(..., description="å®é™…ä½¿ç”¨çš„æ¨¡å¼")
    tokens_estimated: int = Field(default=0, description="ä¼°ç®— token æ¶ˆè€—")
    rule_matched: bool = Field(default=False, description="æ˜¯å¦è§„åˆ™åŒ¹é…")
    tool_used: Optional[str] = Field(None, description="ä½¿ç”¨çš„å·¥å…·")
    fallback_needed: bool = Field(default=False, description="æ˜¯å¦éœ€è¦ fallback åˆ°å®Œæ•´æ¨¡å¼")


@router.post("/fast/chat", response_model=FastChatResponse)
async def fast_chat(
    request: FastChatRequest,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
):
    """
    å¿«é€ŸèŠå¤©æ¥å£ï¼ˆåŒ LLM æ¶æ„ï¼Œçœ Tokenï¼‰
    
    å·¥ä½œæµç¨‹:
    1. è§„åˆ™åŒ¹é… â†’ å·¥å…·æ‰§è¡Œ â†’ æ ¼å¼åŒ–è¿”å› (0 token)
    2. Intent LLM â†’ å·¥å…·æ‰§è¡Œ â†’ Summary LLM (~400 tokens)
    3. å¤æ‚é—®é¢˜ fallback åˆ°å®Œæ•´ LLM
    
    ç›¸æ¯”ä¼ ç»Ÿ Tool Callingï¼ŒToken æ¶ˆè€—é™ä½ 60-70%
    """
    import time
    start_time = time.time()
    logger.info(f"âš¡ [FastChat] å¼€å§‹å¤„ç†: user={current_user.username}, msg={request.message[:50]}...")
    
    config = await get_user_llm_config(current_user.id, db)
    
    # ä½¿ç”¨å…±äº« Agentï¼ˆé¿å…æ¯æ¬¡è¯·æ±‚éƒ½åˆ›å»ºæ–°çš„ HTTP å®¢æˆ·ç«¯ï¼‰
    agent_config = LLMConfig(
        base_url=config.base_url,
        api_key=config.api_key,
        model=config.model,
        intent_model=getattr(settings, 'INTENT_LLM_MODEL', None),
        summary_model=getattr(settings, 'SUMMARY_LLM_MODEL', None),
    )
    
    # ä½¿ç”¨å…±äº« Agent è€Œä¸æ˜¯æ¯æ¬¡åˆ›å»ºæ–°çš„
    agent = await get_shared_agent(agent_config)
    
    try:
        # è§£ææ¨¡å¼
        mode_map = {
            "auto": AgentMode.AUTO,
            "fast": AgentMode.FAST,
            "full": AgentMode.FULL,
        }
        mode = mode_map.get(request.mode, AgentMode.AUTO)
        
        # æ„å»ºç”¨æˆ·ä¸Šä¸‹æ–‡
        user_context = {}
        if request.context:
            if request.context.location:
                user_context["location"] = request.context.location
            if request.context.timezone:
                user_context["timezone"] = request.context.timezone
        
        # å¤„ç†è¯·æ±‚
        result = await agent.process(
            request.message,
            mode=mode,
            skip_summary=request.skip_summary,
            user_context=user_context,
        )
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ fallback
        fallback_needed = (
            result.intent and 
            result.intent.category in (IntentCategory.CHAT, IntentCategory.ANALYZE) and
            not result.content
        )
        
        elapsed = (time.time() - start_time) * 1000
        logger.info(f"âœ… [FastChat] å®Œæˆ: tool={result.intent.tool if result.intent else None}, tokens={result.tokens_estimated}, rule={result.rule_matched}, fallback={fallback_needed}, {elapsed:.0f}ms")
        
        return FastChatResponse(
            content=result.content,
            mode_used=result.mode_used.value,
            tokens_estimated=result.tokens_estimated,
            rule_matched=result.rule_matched,
            tool_used=result.intent.tool if result.intent else None,
            fallback_needed=fallback_needed,
        )
    except Exception as e:
        elapsed = (time.time() - start_time) * 1000
        logger.error(f"âŒ [FastChat] é”™è¯¯: {e}, {elapsed:.0f}ms", exc_info=True)
        raise
    # æ³¨æ„ï¼šä¸å†éœ€è¦ finally ä¸­å…³é—­ agentï¼Œå› ä¸ºä½¿ç”¨çš„æ˜¯å…±äº« Agent


@router.post("/fast/chat/stream")
async def fast_chat_stream(
    request: FastChatRequest,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
):
    """
    å¿«é€ŸèŠå¤©æµå¼æ¥å£ï¼ˆåŒ LLM æ¶æ„ï¼‰
    
    è¿”å› SSE æµï¼ŒåŒ…å«å„é˜¶æ®µçŠ¶æ€:
    - intent: æ„å›¾è¯†åˆ«ç»“æœ
    - tool: å·¥å…·æ‰§è¡ŒçŠ¶æ€
    - content: æœ€ç»ˆå†…å®¹
    - fallback: éœ€è¦åˆ‡æ¢åˆ°å®Œæ•´æ¨¡å¼
    - done: å®Œæˆ
    """
    config = await get_user_llm_config(current_user.id, db)
    
    agent_config = LLMConfig(
        base_url=config.base_url,
        api_key=config.api_key,
        model=config.model,
        intent_model=getattr(settings, 'INTENT_LLM_MODEL', None),
        summary_model=getattr(settings, 'SUMMARY_LLM_MODEL', None),
    )
    
    agent = DualLLMAgent(agent_config)
    
    mode_map = {
        "auto": AgentMode.AUTO,
        "fast": AgentMode.FAST,
        "full": AgentMode.FULL,
    }
    mode = mode_map.get(request.mode, AgentMode.AUTO)
    
    async def generate() -> AsyncGenerator[str, None]:
        try:
            async for event in agent.process_stream(request.message, mode=mode):
                yield f"data: {json.dumps(event, ensure_ascii=False)}\n\n"
        except Exception as e:
            logger.exception("Fast chat stream error")
            yield f"data: {json.dumps({'stage': 'error', 'data': str(e)})}\n\n"
        finally:
            await agent.close()
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache, no-store, must-revalidate",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",  # ç¦ç”¨ Nginx ç¼“å†²
            "Transfer-Encoding": "chunked",
        }
    )


@router.get("/fast/info")
async def get_fast_mode_info(
    current_user: User = Depends(get_current_user),
):
    """
    è·å–å¿«é€Ÿæ¨¡å¼ä¿¡æ¯
    
    è¿”å›æ”¯æŒçš„å·¥å…·åˆ†ç±»å’Œé¢„ä¼° token æ¶ˆè€—å¯¹æ¯”
    """
    return {
        "enabled": getattr(settings, 'DUAL_LLM_ENABLED', True),
        "supported_categories": [
            {"id": "encode", "name": "ç¼–ç ", "examples": ["base64", "url", "html", "hex"]},
            {"id": "decode", "name": "è§£ç ", "examples": ["base64", "url", "html", "hex"]},
            {"id": "hash", "name": "å“ˆå¸Œ", "examples": ["md5", "sha256", "hmac"]},
            {"id": "network", "name": "ç½‘ç»œ", "examples": ["dns", "whois", "ip"]},
        ],
        "fallback_categories": [
            {"id": "analyze", "name": "å®‰å…¨åˆ†æ", "reason": "éœ€è¦å®Œæ•´ LLM èƒ½åŠ›"},
            {"id": "chat", "name": "æ™®é€šå¯¹è¯", "reason": "å¼€æ”¾å¼é—®ç­”"},
        ],
        "token_comparison": {
            "traditional": {"per_request": "1000-2000", "description": "ä¼ ç»Ÿ Tool Calling"},
            "fast_mode": {"per_request": "0-400", "description": "åŒ LLM æ¶æ„"},
            "savings": "60-70%",
        },
        "models": {
            "intent": getattr(settings, 'INTENT_LLM_MODEL', None) or "ä½¿ç”¨é»˜è®¤æ¨¡å‹",
            "summary": getattr(settings, 'SUMMARY_LLM_MODEL', None) or "ä½¿ç”¨é»˜è®¤æ¨¡å‹",
        }
    }
